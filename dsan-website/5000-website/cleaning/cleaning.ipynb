{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "  html:\n",
    "      embed-resources: true\n",
    "      code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all cleaned data [click here](https://github.com/anly501/dsan-5000-project-jsweren1/tree/main/dsan-website/5000-website/data/cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly and Annual Ridership Totals by Mode​ of Transportation [^1]\n",
    "\n",
    "The purpose of this data is to gain a baseline perspective of the current state of public transit usage in the United States. Therefore, this data set should be cleaned in a way that trends can be visualized, without including superfluous information that does not relate to any current phenomena. The steps used in cleaning this data are below.\n",
    "\n",
    "- Trim the data set:\n",
    "  - Columns 1 to 11 to trim blank items in the .csv file, as well as notes put in by the source.\n",
    "  - Rows 81 to 133 to remove records from prior to 2010, as those are superfluous when comparing to current trends.\n",
    "- Create one column to account for year and quarter to improve readability\n",
    "- Convert all numeric rows to numeric data type\n",
    "- Remove extra year and quarter columns as they are now unnecessary\n",
    "\n",
    "Regarding the numeric fields, I have chosen to keep them all for now as each one can provide insight into which modes of transportation are most affected by certain factors. Below is the code to apply the steps laid out, as well as a comparison between the raw and cleaned data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "ridership <- read.csv(\"../data/APTA-Ridership-by-Mode-and-Quarter-1990-Present.csv\")\n",
    "ridership <- ridership[81:133,1:11]\n",
    "colnames(ridership)[2] <- 'Year - Quarter'\n",
    "colnames(ridership)[4:11] <- c(\"total_ridership\", \"heavy_rail\", \"light_rail\", \"commuter_rail\", \"trolleybus\", \"bus\", \"demand_response\", \"other\")\n",
    "ridership$total_ridership <- as.numeric(gsub(\",\",\"\", ridership$total_ridership))\n",
    "ridership$heavy_rail <- as.numeric(gsub(\",\",\"\", ridership$heavy_rail))\n",
    "ridership$light_rail <- as.numeric(gsub(\",\",\"\", ridership$light_rail))\n",
    "ridership$commuter_rail <- as.numeric(gsub(\",\",\"\", ridership$commuter_rail))\n",
    "ridership$trolleybus <- as.numeric(gsub(\",\",\"\", ridership$trolleybus))\n",
    "ridership$bus <- as.numeric(gsub(\",\",\"\", ridership$bus))\n",
    "ridership$demand_response <- as.numeric(gsub(\",\",\"\", ridership$demand_response))\n",
    "ridership$other <- as.numeric(gsub(\",\",\"\", ridership$other))\n",
    "ggplot(data=ridership, aes(x=factor(`Year - Quarter`), y=total_ridership, group=1, xmin = \"2015 - Q1\", xmax=\"2023-Q1\")) +\n",
    "  geom_line()+\n",
    "  geom_point()+\n",
    "  labs(x = \"Year - Quarter\", y = \"Total Ridership (000s)\", title = \"Total Public Transit Ridership in the U.S.\")+\n",
    "  theme(axis.text.x = element_text(angle = 45))\n",
    "ridership <- ridership[c(2, 4:11)]\n",
    "head(ridership)\n",
    "write.csv(ridership, \"../data/cleaned_data/ridership_by_quarter_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Raw Quarterly Ridership Data](../images/apta_raw_data.png)\n",
    "\n",
    "![Cleaned Quarterly Ridership Data](../images/quarterly_ridership_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census Data for Commute to Work [^2]\n",
    "\n",
    "*Note: Due to the size of this data, it will not be hosted on Github. The cleaned data can be accessed using [this link](https://jms819.georgetown.domains/dsan-website/5000-website/data/cleaned_data/).*\n",
    "\n",
    "The main objective of cleaning this data is to narrow down the fields to remove superfluous columns, and to decode the numerical values that the dataset has in place of categorical values. To do this, we will reference the glossary that accompanies the dataset. The steps are the following:\n",
    "\n",
    "- Remove columns that provide excess detail\n",
    "- Rename columns\n",
    "- Remove columns that will not be necessary for any analysis techniques to be used later on\n",
    "- Replace codes for `sex`, `marital_status`, `race`, `hispanic`, `employment`, `metropolitan_status`, and `transportation_type`\n",
    "  - Codes for `metropolitan_status` and `transportation_type` are aggregated to simplify data (e.g., all public transit types are labeled `Public Transit`)\n",
    "- Set `age` and `income` to numerical data types\n",
    "- Set all values where `income` is 0 and the person is not in the labor force to `NA`\n",
    "- Drop all rows where `transportation_type` is `NA`, as those are not labeled\n",
    "- Set all placeholder values for `city_population` to `NA`\n",
    "- Set all placeholder values for `income` to `NA`\n",
    "- Write to `.csv`\n",
    "\n",
    "The code and output are shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "commute <- read.csv(\"../data/ipums_commute.csv\")\n",
    "commute <- commute[-c(6,8,12,15)]\n",
    "colnames(commute) <- c('city_population_00s','sex','age','marital_status','race','hispanic','citizenship','english',\n",
    "                       'employment','labor_force','worker_class', 'income','transportation_type','transportation_time')\n",
    "commute <- commute[,!(names(commute) %in% c('citizenship','english','labor_force','worker_class'))]\n",
    "commute$sex <- replace(commute$sex, commute$sex=='1', 'Male')\n",
    "commute$sex <- replace(commute$sex, commute$sex=='2', 'Female')\n",
    "commute$age <- as.integer(commute$age)\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='1', 'Married present')\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='2', 'Married absent')\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='3', 'Separated')\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='4', 'Divorced')\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='5', 'Widowed')\n",
    "commute$marital_status <- replace(commute$marital_status, commute$marital_status=='6', 'Never married')\n",
    "commute$race <- replace(commute$race, commute$race=='1', 'White')\n",
    "commute$race <- replace(commute$race, commute$race=='2', 'Black')\n",
    "commute$race <- replace(commute$race, commute$race=='3', 'American Indian')\n",
    "commute$race <- replace(commute$race, commute$race=='4', 'Chinese')\n",
    "commute$race <- replace(commute$race, commute$race=='5', 'Japanese')\n",
    "commute$race <- replace(commute$race, commute$race=='6', 'Other Asian or PI')\n",
    "commute$race <- replace(commute$race, commute$race=='7', 'Other race')\n",
    "commute$race <- replace(commute$race, commute$race=='8', 'Two races')\n",
    "commute$race <- replace(commute$race, commute$race=='9', 'Three or more races')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='0', 'Not Hispanic')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='1', 'Mexican')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='2', 'Puerto Rican')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='3', 'Cuban')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='4', 'Other')\n",
    "commute$hispanic <- replace(commute$hispanic, commute$hispanic=='9', 'Not reported')\n",
    "commute$employment <- replace(commute$employment, commute$employment=='0', NA)\n",
    "commute$employment <- replace(commute$employment, commute$employment=='1', 'Employed')\n",
    "commute$employment <- replace(commute$employment, commute$employment=='2', 'Unemployed')\n",
    "commute$employment <- replace(commute$employment, commute$employment=='3', 'Not in labor force')\n",
    "commute$income <- as.integer(commute$income)\n",
    "commute$income <- replace(commute$income,commute$employment=='Not in labor force' & commute$income==0, NA)\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type=='0', NA)\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type %in% c('10','11','12','13','14','15','20'), 'Private Vehicle')\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type %in% c('31','32','33','34','35','36','37','38','39'), 'Public Transit')\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type=='50', 'Bicycle')\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type=='60', 'Walk')\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type=='70', 'Other')\n",
    "commute$transportation_type <- replace(commute$transportation_type, commute$transportation_type=='80', 'Work From Home')\n",
    "commute$city_population_00s <- replace(commute$city_population_00s, commute$city_population_00s %in% c(0,99999), NA)\n",
    "commute$income <- replace(commute$income, commute$income %in% c(-009995,-000001,0000000,0000001,9999999), NA)\n",
    "\n",
    "commute <- commute %>% drop_na(transportation_type)\n",
    "head(commute)\n",
    "write.csv(commute, \"../data/cleaned_data/commute_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Commute by Demographic - Cleaned](../images/commute_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Public Transit Data by City [^3]\n",
    "\n",
    "This data was gathered to attempt to find differences in public transit system performance by city. However, the raw data comes in a slightly different form. The observational unit is mode of transportation, separated by transit agency. For our purposes, we want the observational unit to be each city, so many of these rows must be consolidated. To accomplish this, it is important to understand which values are should be summed (i.e., counting variables), and which should be added as proportions of the total. For each, column, a formula must be applied to ensure proper consolidation. The steps for cleaning this dataset are as follows:\n",
    "\n",
    "- Remove rows with unnecessary information\n",
    "- Remove rows in which the `Most Recent Report Year` is not 2022, the latest year with sufficient data\n",
    "- Initialize new dataframe to insert consolidated rows\n",
    "  - Length equal to the number of unique city names (i.e., number of cities)\n",
    "- Set `Population` to the population value associated with each city\n",
    "- Set `Area` to the area value in square miles associated with each city\n",
    "- Set `Cost_per_trip`, `Fare_per_trip`, and `Miles_per_trip`\n",
    "  - As these are all average values, this is done by multiplying the value for each transportation type by the number of passenger trips to properly weigh that data point, sum all of those values, and divide by the total number of passenger trips for that city\n",
    "- Compute `Trips_per_capita` as total trips divided by population\n",
    "- Write the resulting dataframe to a `.csv` file\n",
    "\n",
    "The code and output from cleaning this dataset are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "library(readxl)\n",
    "\n",
    "cities <- read_excel(\"../data/apta-cities_9-23.xlsx\",sheet = 2)\n",
    "cities <- cities[-c(2,4:6,8:12,16:17,19:20)]\n",
    "cities <- cities[(cities$Status %in% \"Active\" & cities$`Most Recent Report Year` %in% 2022),]\n",
    "unique(cities$Status)\n",
    "head(cities)\n",
    "nrows <- length(unique(cities$`UZA Name`))\n",
    "cities2 <- data.frame(City=character(nrows), Population=numeric(nrows), Area=numeric(nrows),\n",
    "                      Cost_per_trip=numeric(nrows), Fare_per_trip=numeric(nrows), Miles_per_trip=numeric(nrows))\n",
    "cities2$City <- unique(cities$`UZA Name`)\n",
    "for (i in 1:nrows) {\n",
    "  cities2[i,2] <- as.numeric(unique(cities$`UZA Population`[cities$`UZA Name`==cities2[i,1]])[1])\n",
    "}\n",
    "for (i in 1:nrows) {\n",
    "  cities2[i,3] <- round(as.numeric(unique(cities$`UZA SQ Miles`[cities$`UZA Name`==cities2[i,1]])[1]), digits = 2)\n",
    "}\n",
    "for (i in 1:nrows) {\n",
    "  cities2[i,4] <- sum((cities$`Avg Cost Per Trip FY`[cities$`UZA Name`==cities2[i,1]] * cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]])\n",
    "                      / sum(cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]]))\n",
    "}\n",
    "for (i in 1:nrows) {\n",
    "  cities2[i,5] <- sum((cities$`Avg Fares Per Trip FY`[cities$`UZA Name`==cities2[i,1]] * cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]])\n",
    "                      / sum(cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]]))\n",
    "}\n",
    "for (i in 1:nrows) {\n",
    "  cities2[i,6] <- sum((cities$`Avg Trip Length FY`[cities$`UZA Name`==cities2[i,1]] * cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]])\n",
    "                      / sum(cities$`Unlinked Passenger Trips FY`[cities$`UZA Name`==cities2[i,1]]))\n",
    "}\n",
    "cities2$Trips_per_capita <- cities2$Total_trips / cities2$Population\n",
    "head(cities2)\n",
    "write.csv(cities2, \"../data/cleaned_data/apta_cities_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cleaned Cities Data](../images/apta_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews [^4] [^5] [^6] [^7] [^8] [^9] [^10]\n",
    "\n",
    "The purpose of cleaning this data is to perform Naive Bayes classification in the future, as we have labeled text data that can be valuable for analyzing how people express their opinions on public transit systems. In the raw data that was obtained, there are duplicates on each page which must be dealt with, as well as a need for correcting the data types. Since this process must be iterated seven times to account for each transit organization, we will create a generalized fumction to be called upon for each city involved. The steps in this function are:\n",
    "\n",
    "- Remove excess columns\n",
    "- Remove rows where review is duplicated (`date` is `NA` in these records, so we drop based on that)\n",
    "- Add a column to keep track of which agency the review is about\n",
    "- Change column names\n",
    "- Take just the numerical rating and set to integer type\n",
    "- Set Date field to date type\n",
    "- Append to main dataframe using `pd.concat()` function\n",
    "\n",
    "The code for this function is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_yelp(x, y):\n",
    "    df = pd.read_csv('../data/yelp_reviews/' + x + '_reviews.csv')\n",
    "    df = df.drop(columns='Unnamed: 0')\n",
    "    df = df[df['1'].notna()]\n",
    "    df['Agency'] = x\n",
    "    df = df.rename(columns={'0': 'Rating', '1': 'Date', '2': 'Review'})\n",
    "    df['Rating'] = df['Rating'].str[0].astype(int)\n",
    "    for i in df['Date']:\n",
    "        i = datetime.strptime(i, \"%b %d, %Y\")\n",
    "    total = pd.concat([y,df])\n",
    "    return(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will call the function for each of our seven cities. The code and output for this are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_cleaned = pd.DataFrame(columns=['Rating', 'Date', 'Review', 'Agency'])\n",
    "yelp_cleaned = clean_yelp('mta', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('la', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('cta', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('septa', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('mbta', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('bart', yelp_cleaned)\n",
    "yelp_cleaned = clean_yelp('wmata', yelp_cleaned)\n",
    "yelp_cleaned.to_csv('../data/cleaned_data/yelp_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cleaned Yelp Reviews](../images/yelp_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Work Trends - Desires of Employers vs. Workers [^11]\n",
    "\n",
    "The insight to be gathered from this data would be the discrepancies between what employers want from their workers' remote work schedule, and those of the workers themselves. Therefore, while these come from two separate .csv files, it will be necessary to merge these data sets into one data frame. Additionally, each data set has two variables: \n",
    "1. The amount of working from home (days per week) employers or workers want for all workers\n",
    "2. The amount of working from home (days per week) employers or workers want for workers able to work from home\n",
    "Since both of these have ample data, we will keep both. The methodology for this is as follows:\n",
    "\n",
    "- Read both data sets and trim excess space where the owner of the data had included a citation note\n",
    "- Merge by date\n",
    "  - These data sets come from the same series of surveys, so the date column is exactly the same, eliminating any need for removal of rows.\n",
    "- Convert the date field to a date data type and order by date\n",
    "- Rename columns based on glossary provided by the data source\n",
    "- Ensure numeric columns have numeric data type\n",
    "- Remove rows in which there are too many `NA` values.\n",
    "  - Rows in which the values for all workers **OR** workers able to work from home have `NA` values can be kept, as there is a comparison to be made with the ones that don't have `NA` values. Only rows in which no comparison can be made will be removed.\n",
    "\n",
    "The code for this is below, along with a screenshot of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "employer <- read.csv(\"../data/WFH_monthly/WFH_monthly_employer.csv\")\n",
    "worker <- read.csv(\"../data/WFH_monthly/WFH_monthly_worker.csv\")\n",
    "employer <- employer[c(1:3)]\n",
    "worker <- worker[c(1:3)]\n",
    "plans <- merge(employer, worker, by = \"date\")\n",
    "plans$date <- as.Date(plans$date, format = \"%m/%d/%y\")\n",
    "plans <- plans[order(plans$date),]\n",
    "colnames(plans)[c(2:5)] <- c(\"employer_desires_all\", \"employer_desires_able\", \"worker_desires_all\", \"worker_desires_able\")\n",
    "typeof(plans$employer_desires_all)\n",
    "plans <- plans[!(is.na(plans$employer_desires_all) & is.na(plans$employer_desires_able)),]\n",
    "head(plans)\n",
    "write.csv(plans, \"../data/cleaned_data/WFH_surveys_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cleaned Data for Remote Work Plans of Employers and Workers](../images/wfh_plans_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Work Trends by City\n",
    "\n",
    "The cleaning methodology for this data is quite simple since it is largely clean already. The steps are as follows:\n",
    "\n",
    "- Read .csv file and remove all excess columns\n",
    "- Convert the date field to a date data type and order by date\n",
    "- Ensure no `NA` values and that numeric columns have a numeric data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "city <- read.csv(\"../data/WFH_monthly/WFH_monthly_city.csv\")\n",
    "city <- city[c(1, 5:13)]\n",
    "city$date <- as.Date(city$date, format = \"%m/%d/%y\")\n",
    "city <- city[order(city$date),]\n",
    "colnames(city)[c(2:10)] <- c(\"Atlanta\", \"Bay Area\", \"Chicago\", \"DC\", \"Dallas\", \"Houston\", \"Los Angeles\", \"Miami\", \"New York\")\n",
    "city <- na.omit(city)\n",
    "head(city)\n",
    "write.csv(city, \"../data/cleaned_data/WFH_city_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Remote Work Percentages by City - Raw](../images/wfh_city.png)\n",
    "\n",
    "![Remote Work Percentages by City - Cleaned](../images/wfh_city_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: “Ridership Report.” American Public Transportation Association, 21 Sept. 2023, www.apta.com/research-technical-resources/transit-statistics/ridership-report/. \n",
    "\n",
    "[^2]: Steven Ruggles, Sarah Flood, Matthew Sobek, Danika Brockman, Grace Cooper,  Stephanie Richards, and Megan Schouweiler. IPUMS USA: Version 13.0 [dataset]. Minneapolis, MN: IPUMS, 2023.\n",
    "https://doi.org/10.18128/D010.V13.0\n",
    "\n",
    "[^3]: “Raw monthly ridership (no adjustments or estimates),” Raw Monthly Ridership (No Adjustments or Estimates) | FTA, https://www.transit.dot.gov/ntd/data-product/monthly-module-raw-data-release (accessed Nov. 14, 2023).\n",
    "\n",
    "[^4]: “Metropolitan Transportation Authority - New York, NY,” Yelp, https://www.yelp.com/biz/metropolitan-transportation-authority-new-york-6 (accessed Nov. 14, 2023).\n",
    "\n",
    "[^5]: “Metro Los Angeles - Los Angeles, CA,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 14, 2023).\n",
    "\n",
    "[^6]: “Chicago Transit Authority - Chicago, IL,” Yelp, https://www.yelp.com/biz/metro-los-angeles-los-angeles (accessed Nov. 14, 2023).\n",
    "\n",
    "[^7]: “Septa - Philadelphia, PA,” Yelp, https://www.yelp.com/biz/septa-philadelphia-7 (accessed Nov. 14, 2023).\n",
    "\n",
    "[^8]: “Massachusetts Bay Transportation Authority - Boston, MA,” Yelp, https://www.yelp.com/biz/massachusetts-bay-transportation-authority-boston (accessed Nov. 14, 2023).\n",
    "\n",
    "[^9]: “WMATA - Washington, DC, DC,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 2, 2023). \n",
    "\n",
    "[^10]: “Bart - Bay Area Rapid Transit - Oakland, CA,” Yelp, https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2 (accessed Nov. 2, 2023). \n",
    "\n",
    "[^11]: Barrero, Jose Maria, et al. Why Working from Home Will Stick, 2021, https://doi.org/10.3386/w28731."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5000",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
