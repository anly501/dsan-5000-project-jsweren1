[
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Exploratory Data Exploration",
    "section": "",
    "text": "Now that the monthly ridership data from 2018-2023 is cleaned for both WMATA and BART datasets, exploratory data analysis (EDA) can be performed. An important aspect of this data to reiterate is that WMATA’s dataset contains average daily public transit entries for each month, while BART’s dataset simply has a single sample from one day of public transit entries from the 18th day of each month. Therefore, comparison of these trends rely on the assumption that the samples for BART are representative of the month’s average. Nonetheless, viewing these trends alongside each other can provide great insight into the differentiation between cities of the recent phenomena regarding public transit usage.\nTo accomplish this, we will first combine these datasets into a single dataframe and obtain summary statistics. In this case, along with many below, summary statistics are obtained using the pandas software tool in Python, which allows for data manipulation and analysis.\n\n\nCode\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime as dt\nimport pandas as pd\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\nbart_monthly = pd.read_csv('../data/cleaned_data/bart_monthly_ridership.csv')\nwmata_monthly = pd.read_csv('../data/cleaned_data/wmata_monthly_ridership.csv')\ncombined_ridership = bart_monthly.merge(wmata_monthly, on='date')\ncombined_ridership = combined_ridership.drop(columns=['Unnamed: 0_x','Unnamed: 0_y'])\ncombined_ridership = combined_ridership.rename(columns={'daily_entries': 'bart_ridership', 'avg_daily_entries': 'wmata_ridership'})\ncombined_ridership[['bart_ridership','wmata_ridership']].describe()\n#combined_ridership.head()\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\n\n\n\n\n\nbart_ridership\nwmata_ridership\n\n\n\n\ncount\n69.000000\n69.000000\n\n\nmean\n218181.318841\n290734.623188\n\n\nstd\n153902.308664\n176660.259728\n\n\nmin\n25141.000000\n30017.000000\n\n\n25%\n92401.000000\n140034.000000\n\n\n50%\n149296.000000\n237880.000000\n\n\n75%\n407339.000000\n472304.000000\n\n\nmax\n432971.000000\n568265.000000\n\n\n\n\n\n\n\nThere is not much to glean from these statistics prior to visualization, as the numbers can be difficult to comprehend without context. However, we can see that WMATA ridership seems to exceed that of BART consistently. More information can be acquired by plotting the time series of ridership.\n\n\nCode\nplt.plot(combined_ridership['date'], combined_ridership['bart_ridership'], label ='BART')\nplt.plot(combined_ridership['date'], combined_ridership['wmata_ridership'], label ='WMATA')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Daily Ridership\")\nplt.tick_params(bottom=False)\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis graph, and many others below, use the matplotlib software tool in Python, which offers a suite of graphical visualizations to assist us with data understanding.\nBy looking at this time series graph, there are a few key observations to acknowledge. The first is that both systems appeared to be somewhat equally affected by the significant drop in March of 2020, as we’d expect. Additionally, it is clear that prior to this drop, WMATA consistently had more riders each month, a difference that all but disappeared when the pandemic hit. A crucial development to consider is that over the period of recovery, WMATA has regained riders at a faster rate, now matching–or possible exceeding–the gap that had previously been present. Evaluating why these recoveries seem to diverge will be an integral part of future analysis. Finally, it is notable that both trends appear to have not yet reached a plateau, indicating that there may be further increases in ridership to come.\nWe can establish a hypothesis based on this stating that WMATA and BART ridership follows the same trend over the given period of time.\n\n\nCode\ncombined_ridership.corr(method='pearson',numeric_only=True)\n\n\n\n\n\n\n\n\n\nbart_ridership\nwmata_ridership\n\n\n\n\nbart_ridership\n1.000000\n0.976802\n\n\nwmata_ridership\n0.976802\n1.000000\n\n\n\n\n\n\n\nLastly, we can use Pearson correlation analysis to see the correlation between trends of each public transit system. This gives us a value of \\(0.9768\\), which indicates that from 2018-2023, WMATA and BART ridership are significantly positively correlated.\n\n\n\n\n\nCode\nwfh_monthly = pd.read_csv('../data/cleaned_data/WFH_city_cleaned.csv')\nwfh_monthly = wfh_monthly.drop(columns='Unnamed: 0')\nwfh_monthly[['wfh_BayArea','wfh_WashingtonDC']].describe()\n\n\n\n\n\n\n\n\n\nwfh_BayArea\nwfh_WashingtonDC\n\n\n\n\ncount\n35.000000\n35.000000\n\n\nmean\n38.928571\n36.657143\n\n\nstd\n4.243789\n4.792107\n\n\nmin\n32.300000\n30.000000\n\n\n25%\n36.050000\n32.850000\n\n\n50%\n39.300000\n37.900000\n\n\n75%\n41.500000\n39.500000\n\n\nmax\n54.200000\n54.800000\n\n\n\n\n\n\n\nThe first notable piece of information about this data is that it begins in October of 2020, which is well after the pandemic’s effects began taking shape. However, the first data point represents data up to and including the information reached on that date, so it is sensible to think of it as an aggregate for the early pandemic. Thus, we should keep in mind that it may be an outlier, as it contains data from a greater date range, from a time that we might assume would have a greater work-from-home rate. The summary statistics above tell us little about the trends, but we can see that work-from-home in Washington, D.C. and the San Francisco Bay Area are quite similar at first glance.\n\n\nCode\nfrom matplotlib.pyplot import boxplot, xlabel, ylabel\nboxplot(wfh_monthly[['wfh_BayArea','wfh_WashingtonDC']], labels=['Bay Area','Washington, D.C.'])\nplt.xlabel('Metropolitan Area')\nplt.ylabel('WFH Percentage')\n\n\nText(0, 0.5, 'WFH Percentage')\n\n\n\n\n\nThe boxplot above uses the matplotlib tool in Python to visualize descriptive data side-by-side. This confirms the suspicion that there is indeed one outlier point, which is the first observation for each dataset. In further analysis, it will be important to deal with this.\n\n\nCode\n#p1 = wfh_monthly.plot()\n\nplt.plot(wfh_monthly['date'], wfh_monthly['wfh_BayArea'], label ='Bay Area')\nplt.plot(wfh_monthly['date'], wfh_monthly['wfh_WashingtonDC'], label ='Washington D.C.')\nplt.xlabel(\"Date\")\nplt.ylabel(\"WFH Percentage\")\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis line graph also uses matplotlib and shows time series data on how work-from-home percentages have evolved over the course of the pandemic recovery. In both metropolitan areas, we see a steady decline in work-from-home reported, which suggests a general trend of people returning to the office, thus potentially needing public transit. While the differences between these lines are difficult to discern, it does appear that the San Francisco Bay Area’s recent data exceeds that of Washington, D.C., indicating that their residents have retained work-from-home status at a greater rate.\nNext, we will merge this data with the public transit ridership data to conduct correlation analyses and gain insights on the extent of the impact that work-from-home has on ridership.\n\n\nCode\nwfh_ridership = combined_ridership[33:67].merge(wfh_monthly, on='date')\nwfh_ridership.head()\nprint(wfh_ridership.corr(method='pearson',numeric_only=True))\nsns.heatmap(wfh_ridership.corr(numeric_only=True))\n\n\n                  bart_ridership  wmata_ridership  wfh_BayArea   \nbart_ridership          1.000000         0.963119    -0.593388  \\\nwmata_ridership         0.963119         1.000000    -0.645030   \nwfh_BayArea            -0.593388        -0.645030     1.000000   \nwfh_WashingtonDC       -0.756827        -0.790434     0.869830   \n\n                  wfh_WashingtonDC  \nbart_ridership           -0.756827  \nwmata_ridership          -0.790434  \nwfh_BayArea               0.869830  \nwfh_WashingtonDC          1.000000  \n\n\n&lt;Axes: &gt;\n\n\n\n\n\nThe above correlation heatmap uses the seaborn tool in Python, which provides advanced data visualizations for enhanced exploratory analysis. Using this visualization, along with the associated valuesm, we can see that work-from-home rates are heavily positively correlated between cities. This confirms the observation made previously. In addition, work-from-home has a strong negative correlation with public transit ridership for both metropolitan areas. That is, as fewer people work from home, more people use public transit. This is also expected and will serve as a great refined hypothesis for future analysis.\n\n\n\nIn evaluating the future of work-from-home, as well as the social impact it has, it is important to perform EDA on the work-from-home desires of both workers and employers. The summary statistics below show a reality that is expected: workers want more work-from-home days than employers, and both want fewer work-from-home days for workers who are able to work on-site.\n\n\nCode\nwfh_desires = pd.read_csv('../data/cleaned_data/WFH_surveys_cleaned.csv')\nwfh_desires = wfh_desires.drop(columns='Unnamed: 0')\nwfh_desires[['employer_desires_all','employer_desires_able','worker_desires_all','worker_desires_able']].describe()\n\n\n\n\n\n\n\n\n\nemployer_desires_all\nemployer_desires_able\nworker_desires_all\nworker_desires_able\n\n\n\n\ncount\n38.000000\n37.000000\n38.000000\n37.000000\n\n\nmean\n1.359211\n2.045405\n2.270526\n2.807568\n\n\nstd\n0.188628\n0.278932\n0.087763\n0.096650\n\n\nmin\n1.050000\n1.560000\n2.100000\n2.550000\n\n\n25%\n1.177500\n1.800000\n2.190000\n2.770000\n\n\n50%\n1.400000\n2.210000\n2.275000\n2.810000\n\n\n75%\n1.530000\n2.260000\n2.320000\n2.870000\n\n\nmax\n1.610000\n2.370000\n2.470000\n2.970000\n\n\n\n\n\n\n\n\n\nCode\n#p2 = wfh_desires.plot()\nplt.plot(wfh_desires['date'], wfh_desires['employer_desires_all'], label='Employer Desires for All Workers')\nplt.plot(wfh_desires['date'], wfh_desires['employer_desires_able'], label='Employer Desires for Able Workers')\nplt.plot(wfh_desires['date'], wfh_desires['worker_desires_all'], label='Worker Desires for All Workers')\nplt.plot(wfh_desires['date'], wfh_desires['worker_desires_able'], label='Worker Desires for Able Workers')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Days/Week Requested WFH\")\n#p2.xaxis.set_major_locator(mdates.DayLocator(interval=6))\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nUsing this matplotlib-generated plot, we can see time series data for the four categories. The observations noted above remain true, but there is a clear trend that should be pointed out regarding employer desires. While workers seem to have generally maintained the same expected number of work-from-home days, employers have increased their appetite for more work-from-home days. Further analysis should examine causes for this phenomenon, which could be caused by realizations that work-from-home has merit in some cases, a changing political climate, simply conceding ground to their counterparts, or perhaps an undiscovered reason.\nThus, our hypotheses regarding this data set are: 1. Workers desire more work-from-home days/week than employers 2. Work-from-home desires have remained the same over time\n\n\n\nBy performing EDA on hourly public transit usage, we can see how the purpose of public transit ridership has changed since the impacts of the pandemic. It is reasonable to surmise that peaks around “rush hour” are generally related to work commutes, so analyzing the extent of those peaks can provide insight in what the general populus seeks from public transportation. To accomplish this, we can combine the pre-COVID and post-COVID datasets for WMATA hourly ridership. Since we are not interested in comparing total ridership (that was already covered), the first visualization will be a line graph using matplotlib.\n\n\nCode\nhourly_pre = pd.read_csv('../data/cleaned_data/hourly_average_cleaned_pre-covid.csv')\nhourly_post = pd.read_csv('../data/cleaned_data/hourly_average_cleaned_post-covid.csv')\ncombined_hourly = hourly_pre.merge(hourly_post, on='hour')\ncombined_hourly = combined_hourly.drop(columns=['Unnamed: 0_x','Unnamed: 0_y','hour_numeric_x','hour_numeric_y','avg_daily_exits_x','avg_daily_exits_y'])\ncombined_hourly = combined_hourly.rename(columns={'avg_daily_entries_x': 'entries_pre_covid', 'avg_daily_entries_y': 'entries_post_covid'})\nplt.plot(combined_hourly['hour'], combined_hourly['entries_pre_covid'], label = \"Pre-COVID\")\nplt.plot(combined_hourly['hour'], combined_hourly['entries_post_covid'], label = \"Post-COVID\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Average Daily Entries\")\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis graph clearly shows rush hour peaks for both datasets, although it is noteworthy that the peaks appear less pronounced in the post-COVID dataset. This could indicate that commutes to work represented a smaller proportion of total transit ridership after the pandemic hit, which would make intuitive sense. To gain more information, it could be insightful to plot the ratio of entries by hour below.\n\n\nCode\nhourly_ratio = combined_hourly['entries_post_covid']/combined_hourly['entries_pre_covid']\nplt.plot(combined_hourly['hour'], hourly_ratio)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Ratio of Post-COVID to Pre-COVID Entries\")\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis visualization has a few notable features. The first is that the ratio of post-COVID entries to pre-COVID entries does seem to lower around the rush hour times, suggesting that rush hour did have a less pronounced effect later on. Secondly, we can see anomalies in the early-morning hours, where the ratio actually exceeds 1. Due to the relatively small sample of data in these hours, it is fair to call these outliers, as they do not represent overall trends in the data. This presumption is confirmed in the boxplot below.\n\n\nCode\nboxplot(hourly_ratio)\n\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x137ce07d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x137ce1190&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x137ce1b90&gt;,\n  &lt;matplotlib.lines.Line2D at 0x137ce2750&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x137cd3c50&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x137ce3350&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x137ce1150&gt;],\n 'means': []}\n\n\n\n\n\nBased on this EDA, our updated hypothesis regarding this data is that the COVID-19 pandemic had an effect on the distribution of hourly WMATA ridership.\n\n\n\nThis data can provide valuable insight on which forms of public transportation have been most affected. Based on what we can conclude here, there may be indications that some methods should be studied with more emphasis. The summary statistics for this data offer a simple understanding of ridership volume and which methods are used most. However, the bulk of our EDA will focus on correlation between variables to evaluate how these methods’ ridership change alongside one another.\n\n\nCode\nridership_type = pd.read_csv('../data/cleaned_data/ridership_by_quarter_cleaned.csv')\nridership_type = ridership_type.drop(columns='Unnamed: 0')\nridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].describe()\n\n\n\n\n\n\n\n\n\ntotal_ridership\nheavy_rail\nlight_rail\ncommuter_rail\ntrolleybus\nbus\ndemand_response\nother\n\n\n\n\ncount\n5.300000e+01\n5.300000e+01\n53.000000\n53.000000\n53.000000\n5.300000e+01\n53.000000\n53.000000\n\n\nmean\n2.279058e+06\n8.269957e+05\n113075.698113\n104419.962264\n19331.075472\n1.125994e+06\n47151.716981\n42090.377358\n\n\nstd\n5.826479e+05\n2.299050e+05\n30778.435641\n32376.278023\n5935.076336\n2.695039e+05\n10558.336191\n11875.951653\n\n\nmin\n6.209250e+05\n1.222490e+05\n33137.000000\n13521.000000\n3395.000000\n4.205780e+05\n16907.000000\n11140.000000\n\n\n25%\n2.361673e+06\n8.339910e+05\n112616.000000\n109741.000000\n19598.000000\n1.133805e+06\n46331.000000\n40588.000000\n\n\n50%\n2.538958e+06\n9.270570e+05\n125865.000000\n119043.000000\n21395.000000\n1.234299e+06\n51334.000000\n45071.000000\n\n\n75%\n2.624985e+06\n9.682590e+05\n133233.000000\n125486.000000\n23740.000000\n1.312650e+06\n53673.000000\n49803.000000\n\n\nmax\n2.729424e+06\n1.015234e+06\n140828.000000\n130970.000000\n25051.000000\n1.364993e+06\n59535.000000\n57254.000000\n\n\n\n\n\n\n\n\n\nCode\nsns.pairplot(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']], diag_kind='kde', kind=\"scatter\")\n\n\n/Users/joshsweren/anaconda3/envs/dsan5000/lib/python3.11/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nCode\nprint(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].corr(method='pearson',numeric_only=True))\nsns.heatmap(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].corr(numeric_only=True))\n\n\n----------------------\nPEARSON CORRELATION MATRIX:\n----------------------\n                 total_ridership  heavy_rail  light_rail  commuter_rail   \ntotal_ridership         1.000000    0.989721    0.978829       0.979993  \\\nheavy_rail              0.989721    1.000000    0.987208       0.991768   \nlight_rail              0.978829    0.987208    1.000000       0.990598   \ncommuter_rail           0.979993    0.991768    0.990598       1.000000   \ntrolleybus              0.959039    0.923241    0.901781       0.906034   \nbus                     0.987325    0.955084    0.941736       0.940388   \ndemand_response         0.975021    0.976711    0.971167       0.960878   \nother                   0.941060    0.932013    0.933872       0.939074   \n\n                 trolleybus       bus  demand_response     other  \ntotal_ridership    0.959039  0.987325         0.975021  0.941060  \nheavy_rail         0.923241  0.955084         0.976711  0.932013  \nlight_rail         0.901781  0.941736         0.971167  0.933872  \ncommuter_rail      0.906034  0.940388         0.960878  0.939074  \ntrolleybus         1.000000  0.976065         0.908239  0.914181  \nbus                0.976065  1.000000         0.949469  0.920445  \ndemand_response    0.908239  0.949469         1.000000  0.901680  \nother              0.914181  0.920445         0.901680  1.000000  \n\n\n&lt;Axes: &gt;\n\n\n\n\n\nBy using the seaborn package, we can obtain a pairplot, a correlation matrix, and a heatmap to visualize correlation between the ridership of different modes of public transportation. From this, we can see that no Pearson correlation fails to exceed 0.9, indicating strong correlation between all variables. Thus, our refined hypothesis is that all methods of public transportation are impacted equally by the decline and/or resurgence of public transit usage."
  },
  {
    "objectID": "eda/eda.html#data-exploration",
    "href": "eda/eda.html#data-exploration",
    "title": "Exploratory Data Exploration",
    "section": "",
    "text": "Now that the monthly ridership data from 2018-2023 is cleaned for both WMATA and BART datasets, exploratory data analysis (EDA) can be performed. An important aspect of this data to reiterate is that WMATA’s dataset contains average daily public transit entries for each month, while BART’s dataset simply has a single sample from one day of public transit entries from the 18th day of each month. Therefore, comparison of these trends rely on the assumption that the samples for BART are representative of the month’s average. Nonetheless, viewing these trends alongside each other can provide great insight into the differentiation between cities of the recent phenomena regarding public transit usage.\nTo accomplish this, we will first combine these datasets into a single dataframe and obtain summary statistics. In this case, along with many below, summary statistics are obtained using the pandas software tool in Python, which allows for data manipulation and analysis.\n\n\nCode\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nfrom datetime import datetime as dt\nimport pandas as pd\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\nbart_monthly = pd.read_csv('../data/cleaned_data/bart_monthly_ridership.csv')\nwmata_monthly = pd.read_csv('../data/cleaned_data/wmata_monthly_ridership.csv')\ncombined_ridership = bart_monthly.merge(wmata_monthly, on='date')\ncombined_ridership = combined_ridership.drop(columns=['Unnamed: 0_x','Unnamed: 0_y'])\ncombined_ridership = combined_ridership.rename(columns={'daily_entries': 'bart_ridership', 'avg_daily_entries': 'wmata_ridership'})\ncombined_ridership[['bart_ridership','wmata_ridership']].describe()\n#combined_ridership.head()\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\n\n\n\n\n\n\n\n\nbart_ridership\nwmata_ridership\n\n\n\n\ncount\n69.000000\n69.000000\n\n\nmean\n218181.318841\n290734.623188\n\n\nstd\n153902.308664\n176660.259728\n\n\nmin\n25141.000000\n30017.000000\n\n\n25%\n92401.000000\n140034.000000\n\n\n50%\n149296.000000\n237880.000000\n\n\n75%\n407339.000000\n472304.000000\n\n\nmax\n432971.000000\n568265.000000\n\n\n\n\n\n\n\nThere is not much to glean from these statistics prior to visualization, as the numbers can be difficult to comprehend without context. However, we can see that WMATA ridership seems to exceed that of BART consistently. More information can be acquired by plotting the time series of ridership.\n\n\nCode\nplt.plot(combined_ridership['date'], combined_ridership['bart_ridership'], label ='BART')\nplt.plot(combined_ridership['date'], combined_ridership['wmata_ridership'], label ='WMATA')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Daily Ridership\")\nplt.tick_params(bottom=False)\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis graph, and many others below, use the matplotlib software tool in Python, which offers a suite of graphical visualizations to assist us with data understanding.\nBy looking at this time series graph, there are a few key observations to acknowledge. The first is that both systems appeared to be somewhat equally affected by the significant drop in March of 2020, as we’d expect. Additionally, it is clear that prior to this drop, WMATA consistently had more riders each month, a difference that all but disappeared when the pandemic hit. A crucial development to consider is that over the period of recovery, WMATA has regained riders at a faster rate, now matching–or possible exceeding–the gap that had previously been present. Evaluating why these recoveries seem to diverge will be an integral part of future analysis. Finally, it is notable that both trends appear to have not yet reached a plateau, indicating that there may be further increases in ridership to come.\nWe can establish a hypothesis based on this stating that WMATA and BART ridership follows the same trend over the given period of time.\n\n\nCode\ncombined_ridership.corr(method='pearson',numeric_only=True)\n\n\n\n\n\n\n\n\n\nbart_ridership\nwmata_ridership\n\n\n\n\nbart_ridership\n1.000000\n0.976802\n\n\nwmata_ridership\n0.976802\n1.000000\n\n\n\n\n\n\n\nLastly, we can use Pearson correlation analysis to see the correlation between trends of each public transit system. This gives us a value of \\(0.9768\\), which indicates that from 2018-2023, WMATA and BART ridership are significantly positively correlated.\n\n\n\n\n\nCode\nwfh_monthly = pd.read_csv('../data/cleaned_data/WFH_city_cleaned.csv')\nwfh_monthly = wfh_monthly.drop(columns='Unnamed: 0')\nwfh_monthly[['wfh_BayArea','wfh_WashingtonDC']].describe()\n\n\n\n\n\n\n\n\n\nwfh_BayArea\nwfh_WashingtonDC\n\n\n\n\ncount\n35.000000\n35.000000\n\n\nmean\n38.928571\n36.657143\n\n\nstd\n4.243789\n4.792107\n\n\nmin\n32.300000\n30.000000\n\n\n25%\n36.050000\n32.850000\n\n\n50%\n39.300000\n37.900000\n\n\n75%\n41.500000\n39.500000\n\n\nmax\n54.200000\n54.800000\n\n\n\n\n\n\n\nThe first notable piece of information about this data is that it begins in October of 2020, which is well after the pandemic’s effects began taking shape. However, the first data point represents data up to and including the information reached on that date, so it is sensible to think of it as an aggregate for the early pandemic. Thus, we should keep in mind that it may be an outlier, as it contains data from a greater date range, from a time that we might assume would have a greater work-from-home rate. The summary statistics above tell us little about the trends, but we can see that work-from-home in Washington, D.C. and the San Francisco Bay Area are quite similar at first glance.\n\n\nCode\nfrom matplotlib.pyplot import boxplot, xlabel, ylabel\nboxplot(wfh_monthly[['wfh_BayArea','wfh_WashingtonDC']], labels=['Bay Area','Washington, D.C.'])\nplt.xlabel('Metropolitan Area')\nplt.ylabel('WFH Percentage')\n\n\nText(0, 0.5, 'WFH Percentage')\n\n\n\n\n\nThe boxplot above uses the matplotlib tool in Python to visualize descriptive data side-by-side. This confirms the suspicion that there is indeed one outlier point, which is the first observation for each dataset. In further analysis, it will be important to deal with this.\n\n\nCode\n#p1 = wfh_monthly.plot()\n\nplt.plot(wfh_monthly['date'], wfh_monthly['wfh_BayArea'], label ='Bay Area')\nplt.plot(wfh_monthly['date'], wfh_monthly['wfh_WashingtonDC'], label ='Washington D.C.')\nplt.xlabel(\"Date\")\nplt.ylabel(\"WFH Percentage\")\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis line graph also uses matplotlib and shows time series data on how work-from-home percentages have evolved over the course of the pandemic recovery. In both metropolitan areas, we see a steady decline in work-from-home reported, which suggests a general trend of people returning to the office, thus potentially needing public transit. While the differences between these lines are difficult to discern, it does appear that the San Francisco Bay Area’s recent data exceeds that of Washington, D.C., indicating that their residents have retained work-from-home status at a greater rate.\nNext, we will merge this data with the public transit ridership data to conduct correlation analyses and gain insights on the extent of the impact that work-from-home has on ridership.\n\n\nCode\nwfh_ridership = combined_ridership[33:67].merge(wfh_monthly, on='date')\nwfh_ridership.head()\nprint(wfh_ridership.corr(method='pearson',numeric_only=True))\nsns.heatmap(wfh_ridership.corr(numeric_only=True))\n\n\n                  bart_ridership  wmata_ridership  wfh_BayArea   \nbart_ridership          1.000000         0.963119    -0.593388  \\\nwmata_ridership         0.963119         1.000000    -0.645030   \nwfh_BayArea            -0.593388        -0.645030     1.000000   \nwfh_WashingtonDC       -0.756827        -0.790434     0.869830   \n\n                  wfh_WashingtonDC  \nbart_ridership           -0.756827  \nwmata_ridership          -0.790434  \nwfh_BayArea               0.869830  \nwfh_WashingtonDC          1.000000  \n\n\n&lt;Axes: &gt;\n\n\n\n\n\nThe above correlation heatmap uses the seaborn tool in Python, which provides advanced data visualizations for enhanced exploratory analysis. Using this visualization, along with the associated valuesm, we can see that work-from-home rates are heavily positively correlated between cities. This confirms the observation made previously. In addition, work-from-home has a strong negative correlation with public transit ridership for both metropolitan areas. That is, as fewer people work from home, more people use public transit. This is also expected and will serve as a great refined hypothesis for future analysis.\n\n\n\nIn evaluating the future of work-from-home, as well as the social impact it has, it is important to perform EDA on the work-from-home desires of both workers and employers. The summary statistics below show a reality that is expected: workers want more work-from-home days than employers, and both want fewer work-from-home days for workers who are able to work on-site.\n\n\nCode\nwfh_desires = pd.read_csv('../data/cleaned_data/WFH_surveys_cleaned.csv')\nwfh_desires = wfh_desires.drop(columns='Unnamed: 0')\nwfh_desires[['employer_desires_all','employer_desires_able','worker_desires_all','worker_desires_able']].describe()\n\n\n\n\n\n\n\n\n\nemployer_desires_all\nemployer_desires_able\nworker_desires_all\nworker_desires_able\n\n\n\n\ncount\n38.000000\n37.000000\n38.000000\n37.000000\n\n\nmean\n1.359211\n2.045405\n2.270526\n2.807568\n\n\nstd\n0.188628\n0.278932\n0.087763\n0.096650\n\n\nmin\n1.050000\n1.560000\n2.100000\n2.550000\n\n\n25%\n1.177500\n1.800000\n2.190000\n2.770000\n\n\n50%\n1.400000\n2.210000\n2.275000\n2.810000\n\n\n75%\n1.530000\n2.260000\n2.320000\n2.870000\n\n\nmax\n1.610000\n2.370000\n2.470000\n2.970000\n\n\n\n\n\n\n\n\n\nCode\n#p2 = wfh_desires.plot()\nplt.plot(wfh_desires['date'], wfh_desires['employer_desires_all'], label='Employer Desires for All Workers')\nplt.plot(wfh_desires['date'], wfh_desires['employer_desires_able'], label='Employer Desires for Able Workers')\nplt.plot(wfh_desires['date'], wfh_desires['worker_desires_all'], label='Worker Desires for All Workers')\nplt.plot(wfh_desires['date'], wfh_desires['worker_desires_able'], label='Worker Desires for Able Workers')\nplt.xlabel(\"Date\")\nplt.ylabel(\"Days/Week Requested WFH\")\n#p2.xaxis.set_major_locator(mdates.DayLocator(interval=6))\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nUsing this matplotlib-generated plot, we can see time series data for the four categories. The observations noted above remain true, but there is a clear trend that should be pointed out regarding employer desires. While workers seem to have generally maintained the same expected number of work-from-home days, employers have increased their appetite for more work-from-home days. Further analysis should examine causes for this phenomenon, which could be caused by realizations that work-from-home has merit in some cases, a changing political climate, simply conceding ground to their counterparts, or perhaps an undiscovered reason.\nThus, our hypotheses regarding this data set are: 1. Workers desire more work-from-home days/week than employers 2. Work-from-home desires have remained the same over time\n\n\n\nBy performing EDA on hourly public transit usage, we can see how the purpose of public transit ridership has changed since the impacts of the pandemic. It is reasonable to surmise that peaks around “rush hour” are generally related to work commutes, so analyzing the extent of those peaks can provide insight in what the general populus seeks from public transportation. To accomplish this, we can combine the pre-COVID and post-COVID datasets for WMATA hourly ridership. Since we are not interested in comparing total ridership (that was already covered), the first visualization will be a line graph using matplotlib.\n\n\nCode\nhourly_pre = pd.read_csv('../data/cleaned_data/hourly_average_cleaned_pre-covid.csv')\nhourly_post = pd.read_csv('../data/cleaned_data/hourly_average_cleaned_post-covid.csv')\ncombined_hourly = hourly_pre.merge(hourly_post, on='hour')\ncombined_hourly = combined_hourly.drop(columns=['Unnamed: 0_x','Unnamed: 0_y','hour_numeric_x','hour_numeric_y','avg_daily_exits_x','avg_daily_exits_y'])\ncombined_hourly = combined_hourly.rename(columns={'avg_daily_entries_x': 'entries_pre_covid', 'avg_daily_entries_y': 'entries_post_covid'})\nplt.plot(combined_hourly['hour'], combined_hourly['entries_pre_covid'], label = \"Pre-COVID\")\nplt.plot(combined_hourly['hour'], combined_hourly['entries_post_covid'], label = \"Post-COVID\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Average Daily Entries\")\nplt.legend()\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis graph clearly shows rush hour peaks for both datasets, although it is noteworthy that the peaks appear less pronounced in the post-COVID dataset. This could indicate that commutes to work represented a smaller proportion of total transit ridership after the pandemic hit, which would make intuitive sense. To gain more information, it could be insightful to plot the ratio of entries by hour below.\n\n\nCode\nhourly_ratio = combined_hourly['entries_post_covid']/combined_hourly['entries_pre_covid']\nplt.plot(combined_hourly['hour'], hourly_ratio)\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Ratio of Post-COVID to Pre-COVID Entries\")\nplt.gcf().autofmt_xdate()\nplt.show()\n\n\n\n\n\nThis visualization has a few notable features. The first is that the ratio of post-COVID entries to pre-COVID entries does seem to lower around the rush hour times, suggesting that rush hour did have a less pronounced effect later on. Secondly, we can see anomalies in the early-morning hours, where the ratio actually exceeds 1. Due to the relatively small sample of data in these hours, it is fair to call these outliers, as they do not represent overall trends in the data. This presumption is confirmed in the boxplot below.\n\n\nCode\nboxplot(hourly_ratio)\n\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x137ce07d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x137ce1190&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x137ce1b90&gt;,\n  &lt;matplotlib.lines.Line2D at 0x137ce2750&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x137cd3c50&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x137ce3350&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x137ce1150&gt;],\n 'means': []}\n\n\n\n\n\nBased on this EDA, our updated hypothesis regarding this data is that the COVID-19 pandemic had an effect on the distribution of hourly WMATA ridership.\n\n\n\nThis data can provide valuable insight on which forms of public transportation have been most affected. Based on what we can conclude here, there may be indications that some methods should be studied with more emphasis. The summary statistics for this data offer a simple understanding of ridership volume and which methods are used most. However, the bulk of our EDA will focus on correlation between variables to evaluate how these methods’ ridership change alongside one another.\n\n\nCode\nridership_type = pd.read_csv('../data/cleaned_data/ridership_by_quarter_cleaned.csv')\nridership_type = ridership_type.drop(columns='Unnamed: 0')\nridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].describe()\n\n\n\n\n\n\n\n\n\ntotal_ridership\nheavy_rail\nlight_rail\ncommuter_rail\ntrolleybus\nbus\ndemand_response\nother\n\n\n\n\ncount\n5.300000e+01\n5.300000e+01\n53.000000\n53.000000\n53.000000\n5.300000e+01\n53.000000\n53.000000\n\n\nmean\n2.279058e+06\n8.269957e+05\n113075.698113\n104419.962264\n19331.075472\n1.125994e+06\n47151.716981\n42090.377358\n\n\nstd\n5.826479e+05\n2.299050e+05\n30778.435641\n32376.278023\n5935.076336\n2.695039e+05\n10558.336191\n11875.951653\n\n\nmin\n6.209250e+05\n1.222490e+05\n33137.000000\n13521.000000\n3395.000000\n4.205780e+05\n16907.000000\n11140.000000\n\n\n25%\n2.361673e+06\n8.339910e+05\n112616.000000\n109741.000000\n19598.000000\n1.133805e+06\n46331.000000\n40588.000000\n\n\n50%\n2.538958e+06\n9.270570e+05\n125865.000000\n119043.000000\n21395.000000\n1.234299e+06\n51334.000000\n45071.000000\n\n\n75%\n2.624985e+06\n9.682590e+05\n133233.000000\n125486.000000\n23740.000000\n1.312650e+06\n53673.000000\n49803.000000\n\n\nmax\n2.729424e+06\n1.015234e+06\n140828.000000\n130970.000000\n25051.000000\n1.364993e+06\n59535.000000\n57254.000000\n\n\n\n\n\n\n\n\n\nCode\nsns.pairplot(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']], diag_kind='kde', kind=\"scatter\")\n\n\n/Users/joshsweren/anaconda3/envs/dsan5000/lib/python3.11/site-packages/seaborn/axisgrid.py:123: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\n\nCode\nprint(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].corr(method='pearson',numeric_only=True))\nsns.heatmap(ridership_type[['total_ridership','heavy_rail','light_rail','commuter_rail','trolleybus','bus','demand_response','other']].corr(numeric_only=True))\n\n\n----------------------\nPEARSON CORRELATION MATRIX:\n----------------------\n                 total_ridership  heavy_rail  light_rail  commuter_rail   \ntotal_ridership         1.000000    0.989721    0.978829       0.979993  \\\nheavy_rail              0.989721    1.000000    0.987208       0.991768   \nlight_rail              0.978829    0.987208    1.000000       0.990598   \ncommuter_rail           0.979993    0.991768    0.990598       1.000000   \ntrolleybus              0.959039    0.923241    0.901781       0.906034   \nbus                     0.987325    0.955084    0.941736       0.940388   \ndemand_response         0.975021    0.976711    0.971167       0.960878   \nother                   0.941060    0.932013    0.933872       0.939074   \n\n                 trolleybus       bus  demand_response     other  \ntotal_ridership    0.959039  0.987325         0.975021  0.941060  \nheavy_rail         0.923241  0.955084         0.976711  0.932013  \nlight_rail         0.901781  0.941736         0.971167  0.933872  \ncommuter_rail      0.906034  0.940388         0.960878  0.939074  \ntrolleybus         1.000000  0.976065         0.908239  0.914181  \nbus                0.976065  1.000000         0.949469  0.920445  \ndemand_response    0.908239  0.949469         1.000000  0.901680  \nother              0.914181  0.920445         0.901680  1.000000  \n\n\n&lt;Axes: &gt;\n\n\n\n\n\nBy using the seaborn package, we can obtain a pairplot, a correlation matrix, and a heatmap to visualize correlation between the ridership of different modes of public transportation. From this, we can see that no Pearson correlation fails to exceed 0.9, indicating strong correlation between all variables. Thus, our refined hypothesis is that all methods of public transportation are impacted equally by the decline and/or resurgence of public transit usage."
  },
  {
    "objectID": "decision_trees/classification/classification.html",
    "href": "decision_trees/classification/classification.html",
    "title": "Classification",
    "section": "",
    "text": "Build out your website tab for “Classification”"
  },
  {
    "objectID": "decision_trees/regression/regression.html",
    "href": "decision_trees/regression/regression.html",
    "title": "Regression",
    "section": "",
    "text": "Build out your website tab for “Regression”"
  },
  {
    "objectID": "introduction/introduction.html",
    "href": "introduction/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Large public transit networks have been a staple of large cities for over a century. However, in large part due to the ramifications of COVID-19, urban areas across the United States of America have seen significant drops in public transit usage. The graph below uses data from the American Public Transportation Association Quarterly Ridership Report1 to show this unprecedented drop and subsequent partial recovery. This will serve as a generalization and basis for the issue which this paper explores.\n\n\n\nTotal Public Transit Ridership in the U.S. Since 2010: Data from American Public Transportation Association Quarterly Ridership Report\n\n\nWhile the plot above shows that this phenomenon has slightly abated as cities have returned to a resemblance of their previous state, usage numbers still significantly lag behind those from before the pandemic. As large portions of state and municipal budgets go to the creation, improvement, and operation of public transit systems, many have started to question the efficacy of urban bus and train networks in the new post-COVID-19 era.\nThe potential consequences of this decline in usage are easy to see. Many believe this calls for decreased funding or elimination of unprofitable public transit, as money should be allocated towards areas where more people will benefit. On the other hand, the presence of affordable and reliable transportation is often seen as a public service, where monetary loss can be accepted for the good of a city and its residents. Ultimately, the extent to which this belief outweighs current and future struggles can be debated indefinitely. Thus, it is important for us to understand the causes of the recent decline and current recovery, what we can expect from public transit usage in the future, and how public transit impacts the city and its residents. The goal of this paper is to explore these topics and respond to the hypothesis that public transit systems still have efficacy because they remain vital staples of their city with great enough long-term demand to offset temporary financial losses.\n\n\n\n\nTo what extent does the public service of public transit offset financial losses?\nWhat are the primary reason for people to use public transit?\nWhat were the primary causes for the rapid decline in public transit usage? (i.e., what specific COVID-19 ramifications affected it the most?)\nWhat are the primary causes for the slow recovery in public transit usage?\nTo what extent is the decline in public transit usage permanent?\nHow do recent trends in public transit usage differ between cities?\nHow do recent trends in public transit usage differ between modes of transportation?\nWhat political incentives exist regarding funding, eliminating, or expanding public transit?\nTo what extent do residents typically benefit from increased spending on public transit?\nWho is most affected by changes in public transit ridership and policy?\n\n\n\n\nTo properly answer the questions above, it is important to understand work done by other researchers. Initially, there are two academic publications that provide significant insight into public transit phenomena in 2020. Understanding the events and results of the decline in public transit usage will ultimately inform our understanding of how current events shape public behavior, the role of public transit in urban areas, and the extent to which we can predict a rebound in public transit usage.\n\n\nThis publication from September 2020 provides significant insight into what can be observed by studying mobility during the COVID-19 pandemic. Despite a specific focus on Poland, we are able to see how human behavior can be measured and how public transit systems are affected by current events. The paper notes that in addition to studying the affect of a pandemic on public transit, the opposite relationship can occur, where public transportation methods can negatively impact the mitigation of a pandemic. Ultimately, the paper aims to assess the relationships between public transit usage and COVID-19 cases, as well as the former with government stringency.\nIn its methodology, the researchers developed quantifiable metrics to determine government stringency (called Stringency Index) at various times by logging different events of lockdowns, restrictions, and warnings. This, along with already quantified data regarding the status of the pandemic across Poland, was measured alongside mobility metrics such as public transit usage. In the image below, we can see how a relationship was determined through the variables that had been set:\n\n\n\nMobility Changes and Stringency Index\n\n\nUltimately, researchers concluded through analysis of all regions of Poland that despite a relatively mild outbreak of the pandemic, “a combination of government lockdown and social fears of contracting and spreading COVID-19 has caused a substantial reduction in passengers’ public transport demand in Poland.” Meanwhile, a similar relationship between the status of the pandemic (i.e., total cases) and public transit usage. This offers compelling insight for our topic, as it enforces the need to compare different forces in why people may hesitate to use public transit. Additionally, the importance of political decisions are heavily weighed, which turned out to be a telling factor in determining human behavior.\n\n\n\nMuch like the previous article, this publication studied a decline in public transit ridership prior to any eventual rebound. Its data sources are primarily from a widely used transit application to measure changes in demand over time. To accomplish this, the researchers define three key parameters: “base value, the apparent minimal level of demand; cliff and base point, representing the initial date when decline in transit demand began and the final date when decline attenuated, respectively; and decay rate, representing the speed of the demand decline.” Using these metrics, they managed to determine that the impact of COVID-19 on public transit was uneven, often explained by differences in social status.\nBy studying several cities across the United States, this study identified demographic influences in how public transit ridership was affected by COVID-19. Communities with larger minority populations often saw higher base values and less significant declines in ridership, a relationship that was also present for populations that searched for “Coronavirus” less. These communities also correlated heavily with the proportion of essential workers. Ultimately, this indicates that demand for public transit is discretionary. People continue to use public transportation if their occupations and livelihood still necessitate its use.\nThis relates to our questions for the future because it indicates factors that impact people’s public transit usage, many of which are still changing today. In a world where “work-from-home” remains more prevalent than prior to the pandemic, it stands to reason that usage may not fully rebound until people feel it is necessary, not just convenient or inexpensive. Additionally, this publication highlights public transit as a critical infrastructure for underprivileged social groups, which is a necessary consideration when assessing its efficacy."
  },
  {
    "objectID": "introduction/introduction.html#abstract",
    "href": "introduction/introduction.html#abstract",
    "title": "Introduction",
    "section": "",
    "text": "Large public transit networks have been a staple of large cities for over a century. However, in large part due to the ramifications of COVID-19, urban areas across the United States of America have seen significant drops in public transit usage. The graph below uses data from the American Public Transportation Association Quarterly Ridership Report1 to show this unprecedented drop and subsequent partial recovery. This will serve as a generalization and basis for the issue which this paper explores.\n\n\n\nTotal Public Transit Ridership in the U.S. Since 2010: Data from American Public Transportation Association Quarterly Ridership Report\n\n\nWhile the plot above shows that this phenomenon has slightly abated as cities have returned to a resemblance of their previous state, usage numbers still significantly lag behind those from before the pandemic. As large portions of state and municipal budgets go to the creation, improvement, and operation of public transit systems, many have started to question the efficacy of urban bus and train networks in the new post-COVID-19 era.\nThe potential consequences of this decline in usage are easy to see. Many believe this calls for decreased funding or elimination of unprofitable public transit, as money should be allocated towards areas where more people will benefit. On the other hand, the presence of affordable and reliable transportation is often seen as a public service, where monetary loss can be accepted for the good of a city and its residents. Ultimately, the extent to which this belief outweighs current and future struggles can be debated indefinitely. Thus, it is important for us to understand the causes of the recent decline and current recovery, what we can expect from public transit usage in the future, and how public transit impacts the city and its residents. The goal of this paper is to explore these topics and respond to the hypothesis that public transit systems still have efficacy because they remain vital staples of their city with great enough long-term demand to offset temporary financial losses."
  },
  {
    "objectID": "introduction/introduction.html#questions-to-answer",
    "href": "introduction/introduction.html#questions-to-answer",
    "title": "Introduction",
    "section": "",
    "text": "To what extent does the public service of public transit offset financial losses?\nWhat are the primary reason for people to use public transit?\nWhat were the primary causes for the rapid decline in public transit usage? (i.e., what specific COVID-19 ramifications affected it the most?)\nWhat are the primary causes for the slow recovery in public transit usage?\nTo what extent is the decline in public transit usage permanent?\nHow do recent trends in public transit usage differ between cities?\nHow do recent trends in public transit usage differ between modes of transportation?\nWhat political incentives exist regarding funding, eliminating, or expanding public transit?\nTo what extent do residents typically benefit from increased spending on public transit?\nWho is most affected by changes in public transit ridership and policy?"
  },
  {
    "objectID": "introduction/introduction.html#relevant-publications",
    "href": "introduction/introduction.html#relevant-publications",
    "title": "Introduction",
    "section": "",
    "text": "To properly answer the questions above, it is important to understand work done by other researchers. Initially, there are two academic publications that provide significant insight into public transit phenomena in 2020. Understanding the events and results of the decline in public transit usage will ultimately inform our understanding of how current events shape public behavior, the role of public transit in urban areas, and the extent to which we can predict a rebound in public transit usage.\n\n\nThis publication from September 2020 provides significant insight into what can be observed by studying mobility during the COVID-19 pandemic. Despite a specific focus on Poland, we are able to see how human behavior can be measured and how public transit systems are affected by current events. The paper notes that in addition to studying the affect of a pandemic on public transit, the opposite relationship can occur, where public transportation methods can negatively impact the mitigation of a pandemic. Ultimately, the paper aims to assess the relationships between public transit usage and COVID-19 cases, as well as the former with government stringency.\nIn its methodology, the researchers developed quantifiable metrics to determine government stringency (called Stringency Index) at various times by logging different events of lockdowns, restrictions, and warnings. This, along with already quantified data regarding the status of the pandemic across Poland, was measured alongside mobility metrics such as public transit usage. In the image below, we can see how a relationship was determined through the variables that had been set:\n\n\n\nMobility Changes and Stringency Index\n\n\nUltimately, researchers concluded through analysis of all regions of Poland that despite a relatively mild outbreak of the pandemic, “a combination of government lockdown and social fears of contracting and spreading COVID-19 has caused a substantial reduction in passengers’ public transport demand in Poland.” Meanwhile, a similar relationship between the status of the pandemic (i.e., total cases) and public transit usage. This offers compelling insight for our topic, as it enforces the need to compare different forces in why people may hesitate to use public transit. Additionally, the importance of political decisions are heavily weighed, which turned out to be a telling factor in determining human behavior.\n\n\n\nMuch like the previous article, this publication studied a decline in public transit ridership prior to any eventual rebound. Its data sources are primarily from a widely used transit application to measure changes in demand over time. To accomplish this, the researchers define three key parameters: “base value, the apparent minimal level of demand; cliff and base point, representing the initial date when decline in transit demand began and the final date when decline attenuated, respectively; and decay rate, representing the speed of the demand decline.” Using these metrics, they managed to determine that the impact of COVID-19 on public transit was uneven, often explained by differences in social status.\nBy studying several cities across the United States, this study identified demographic influences in how public transit ridership was affected by COVID-19. Communities with larger minority populations often saw higher base values and less significant declines in ridership, a relationship that was also present for populations that searched for “Coronavirus” less. These communities also correlated heavily with the proportion of essential workers. Ultimately, this indicates that demand for public transit is discretionary. People continue to use public transportation if their occupations and livelihood still necessitate its use.\nThis relates to our questions for the future because it indicates factors that impact people’s public transit usage, many of which are still changing today. In a world where “work-from-home” remains more prevalent than prior to the pandemic, it stands to reason that usage may not fully rebound until people feel it is necessary, not just convenient or inexpensive. Additionally, this publication highlights public transit as a critical infrastructure for underprivileged social groups, which is a necessary consideration when assessing its efficacy."
  },
  {
    "objectID": "introduction/introduction.html#footnotes",
    "href": "introduction/introduction.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Quarterly and Annual Totals by Mode​​​,” American Public Transportation Association, https://www.apta.com/research-technical-resources/transit-statistics/ridership-report/ (accessed Oct. 5, 2023).↩︎\nM. Wielechowski, K. Czech, and Ł. Grzęda, “Decline in mobility: Public transport in Poland in the time of the COVID-19 pandemic,” Economies, vol. 8, no. 4, p. 78, 2020. doi:10.3390/economies8040078↩︎\nL. Liu, H. J. Miller, and J. Scheff, “The impacts of covid-19 pandemic on public transit demand in the United States,” PLOS ONE, vol. 15, no. 11, 2020. doi:10.1371/journal.pone.0242476↩︎"
  },
  {
    "objectID": "conclusions/conclusions.html",
    "href": "conclusions/conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "Build out your website tab for “conclusions”"
  },
  {
    "objectID": "clustering/clustering.html#theory",
    "href": "clustering/clustering.html#theory",
    "title": "Clustering",
    "section": "Theory",
    "text": "Theory"
  },
  {
    "objectID": "clustering/clustering.html#methods",
    "href": "clustering/clustering.html#methods",
    "title": "Clustering",
    "section": "Methods",
    "text": "Methods\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\n\ndef generate_2D_normal_clusters(points_per_cluster=200, num_cluster=3, correlated=True):\n    #DATA GENERATION PARAMETERS \n\n    sets={}\n    #GENERATE CLUSTERS FROM 2D NORMAL DISTANCE\n    for cluster in range(0,num_cluster):\n        # points_per_cluster=int(np.random.uniform(low=100, high=300, size=1))\n\n        #normal distribution param\n        L=10\n        ux=np.random.uniform(-L,L,size=1)[0]; \n        uy=np.random.uniform(-L,L,size=1)[0];\n        sx=np.random.uniform(0.1*L,0.25*L,size=1)[0]; \n        sy=np.random.uniform(0.1*L,0.25*L,size=1)[0];           #STD-DEV\n        rho=np.random.uniform(0.0,.99,size=1)[0];       #[0,1) RHO=PEARSON CORRELATION\n        u=np.array([ux,uy])\n        S=np.array([[sx**2.0,rho*sy*sx],[rho*sy*sx,sy**2.0]])   #COVARIANCE METRIC\n        if(correlated==False): \n            S[0,1]=0; S[1,0]=0\n        \n        x1, x2 = np.random.multivariate_normal(u, S, points_per_cluster).T\n\n        if cluster==0:\n            X=np.array([x1,x2]).T\n            y=cluster*np.ones(points_per_cluster)\n        else: \n            X=np.concatenate((X,np.array([x1,x2]).T),axis=0)\n            y=np.concatenate((y,cluster*np.ones(points_per_cluster)),axis=0)\n\n    return X,y\n\n# GENERATE DATA\nnum_clusters_real=1+int(7*np.random.uniform(0,1))\nprint(num_clusters_real)\nX,y = generate_2D_normal_clusters(points_per_cluster=200, num_cluster=num_clusters_real, correlated=True)\nprint(X.shape,y.shape)\n\n#FORCE CONTIGUOUS\nX=np.ascontiguousarray(X)\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n5\n(1000, 2) (1000,)"
  },
  {
    "objectID": "clustering/clustering.html#results",
    "href": "clustering/clustering.html#results",
    "title": "Clustering",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "clustering/clustering.html#conclusions",
    "href": "clustering/clustering.html#conclusions",
    "title": "Clustering",
    "section": "Conclusions",
    "text": "Conclusions"
  },
  {
    "objectID": "code/code.html",
    "href": "code/code.html",
    "title": "Code",
    "section": "",
    "text": "Link to Github repository"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#code-implementation",
    "href": "dimensionality_reduction/dimensionality_reduction.html#code-implementation",
    "title": "Dimensionality Reduction",
    "section": "Code Implementation",
    "text": "Code Implementation\n\n\nCode\nimport pandas as pd\ncities = pd.read_csv('../data/cleaned_data/apta_cities_cleaned.csv')\ncities = cities.drop(columns=['Unnamed: 0'])\ncities.head()\n\n\n\n\n\n\n\n\n\nCity\nPopulation\nArea\nCost_per_trip\nFare_per_trip\nMiles_per_trip\n\n\n\n\n0\nSeattle--Tacoma, WA\n3544011\n982.52\n13.906032\n1.570667\n5.786344\n\n\n1\nSpokane, WA\n447279\n171.67\n13.433827\n0.988308\n4.772569\n\n\n2\nYakima, WA\n133145\n55.77\n19.720093\n1.112531\n5.179168\n\n\n3\nEugene, OR\n270179\n73.49\n10.851494\n2.753356\n3.684118\n\n\n4\nPortland, OR--WA\n2104238\n519.30\n10.804361\n1.025659\n4.011388\n\n\n\n\n\n\n\n\n\nCode\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nX = cities.drop(columns=['City']).to_numpy()\n\nprint('NUMERIC MEAN:\\n',np.mean(X,axis=0))\nprint(\"X SHAPE\",X.shape)\nprint(\"NUMERIC COV:\")\nprint(np.cov(X.T))\n\nfrom numpy import linalg as LA\nw, v1 = LA.eig(np.cov(X.T))\nprint(\"\\nCOV EIGENVALUES:\",w)\nprint(\"COV EIGENVECTORS (across rows):\")\nprint(v1.T)\n\n\nNUMERIC MEAN:\n [7.63817374e+05 2.54954371e+02 1.62164796e+01 1.69764181e+00\n 6.02033451e+00]\nX SHAPE (286, 5)\nNUMERIC COV:\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.[[ 3.00348049e+12  6.21976504e+08 -2.68509287e+06 -2.80714130e+05\n  -3.74964286e+05]\n [ 6.21976504e+08  1.60662563e+05 -7.09356971e+02 -8.34065149e+01\n  -1.18548286e+02]\n [-2.68509287e+06 -7.09356971e+02  1.12521769e+02  1.14344019e+01\n   1.54309551e+01]\n [-2.80714130e+05 -8.34065149e+01  1.14344019e+01  1.07182685e+01\n   6.17463373e+00]\n [-3.74964286e+05 -1.18548286e+02  1.54309551e+01  6.17463373e+00\n   2.61947556e+01]]\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n\nCOV EIGENVALUES: [3.00348062e+12 3.18612161e+04 1.13357226e+02 2.46043548e+01\n 8.18652852e+00]\nCOV EIGENVECTORS (across rows):\n[[-9.99999979e-01 -2.07085246e-04  8.93993769e-07  9.34629441e-08\n   1.24843257e-07]\n [ 2.07087147e-04 -9.99987172e-01  4.82944608e-03  7.95476612e-04\n   1.28713329e-03]\n [-1.36812764e-07  5.03927972e-03  9.77725251e-01  1.15731143e-01\n   1.75026404e-01]\n [ 1.31093957e-07 -4.57011124e-04  1.99629231e-01 -2.56090918e-01\n  -9.45814677e-01]\n [ 2.27827260e-08 -9.92273370e-05  6.46388484e-02 -9.59699490e-01\n   2.73493506e-01]]\n\n\n\n\nCode\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=5)\npca.fit(X)\nprint('\\nPCA')\nprint(pca.components_)\n\n\n\nPCA\n[[ 9.99999979e-01  2.07085246e-04 -8.93993769e-07 -9.34629441e-08\n  -1.24843257e-07]\n [-2.07087147e-04  9.99987172e-01 -4.82944608e-03 -7.95476612e-04\n  -1.28713329e-03]\n [-1.36812764e-07  5.03927972e-03  9.77725251e-01  1.15731143e-01\n   1.75026404e-01]\n [-1.31093957e-07  4.57011124e-04 -1.99629231e-01  2.56090918e-01\n   9.45814677e-01]\n [-2.27827260e-08  9.92273370e-05 -6.46388484e-02  9.59699490e-01\n  -2.73493506e-01]]\n\n\n\n\nCode\nv2=pca.components_\n\nprint(v1/v2)\npca = PCA(n_components=5)\nX1=pca.fit_transform(X)\nplt.scatter(X1[:,0], X1[:,1])\nplt.show()\n\n\n[[-1.          1.00000918  0.15303548 -1.4026303  -0.18249064]\n [ 0.99999082 -1.         -1.0434488   0.57451233  0.07709173]\n [-6.5344325   0.95836039  1.          1.72493959  0.36930913]\n [-0.71294624  1.74060667 -0.57973045 -1.         -1.01468027]\n [-5.47973306 12.97155936 -2.7077587  -0.98553212 -1.        ]]\n\n\n\n\n\n\n\nCode\nfrom sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=1).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:4,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.show()\n\n\nRESULTS\nshape :  (286, 2)\nFirst few points : \n [[ 13.150954   57.8999   ]\n [ 23.046926  -37.931778 ]\n [-45.15894     3.9606774]\n [ 64.51022   -35.225803 ]]\n\n\n\n\n\n\n\nCode\nX_embedded = TSNE(n_components=2, learning_rate='auto',init='random', perplexity=10).fit_transform(X)\n\n# EXPLORE RESULTS\nprint(\"RESULTS\") \nprint(\"shape : \",X_embedded.shape)\nprint(\"First few points : \\n\",X_embedded[0:4,:])\n\n# PLOT \nplt.scatter(X_embedded[:,0],X_embedded[:,1], alpha=0.5)\nplt.show()\n\n\nRESULTS\nshape :  (286, 2)\nFirst few points : \n [[-42.433853    10.063747  ]\n [ -3.3064377   21.02925   ]\n [  0.53624296 -41.15471   ]\n [ 26.222113    13.95583   ]]"
  },
  {
    "objectID": "dimensionality_reduction/dimensionality_reduction.html#reporting",
    "href": "dimensionality_reduction/dimensionality_reduction.html#reporting",
    "title": "Dimensionality Reduction",
    "section": "Reporting",
    "text": "Reporting"
  },
  {
    "objectID": "naive_bayes/naive_bayes.html",
    "href": "naive_bayes/naive_bayes.html",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a group of supervised learning algorithms which use Bayes’ theorem as their foundation. The “naive” aspect of these algorithms is the assumption that every pair of features are conditionally independent. 1 This can be better understood via the Bayes’ Theorem formula and how it applies in Naive Bayes. Given an output variable \\(y\\) and several feature variables \\(x_i\\), Bayes’ Theorem posits the following:\n\\(P\\left(y | x_1, \\ldots, x_n\\right)=\\frac{P(y) P\\left(x_1, \\ldots, x_n | y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\)\nThe assumption that all feature variables are conditionally independent can be represented with the formula\n\\(P\\left(x_i \\mid y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n\\right)=P\\left(x_i \\mid y\\right)\\)\nwhich essentially states that the values of each \\(x_i\\) depend solely on \\(y\\), not the values from any other feature variable. Thus, we can substitute the products of all conditional probabilities into Bayes’ Theorem to represent the relationship as\n\\(P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\).\nUsing this foundation, Naive Bayes classification algorithms attempt to establish classifiers for a dataset by considering values for each feature and assigning probabilities that it belongs in a certain class of the specified output variable. To do this, the algorithm will first take in data from a test dataset. When executing Naive Bayes classification, it is generally recommended to partition cleaned data such that 80% of it belongs in the test dataset. The algorithm will iterate over the feature variables and observe the output, ultimately creating a model for the extents to which the feature variables predict the output classifications.\nDue to the foundation being on Bayes’ Theorem, these methods are probabilistic in nature. That is, rather than being able to decisively classify new data, the algorithms work based on conditional probabilities that records belong to a particular class.\nDiscerning when to use the different variants of Naive Bayes classification is crucial for obtaining meaningful results. Firstly, Gaussian Naive Bayes (GNB) assumes that the feature variables follow a normal distribution. Thus, if we have data that we know comes from a normally distributed population, we can use the appropriate likelihood function for parameter estimation. The same applies for other methods of Naive Bayes classification. Other popular algorithms include Multinomial Naive Bayes, Bernoulli Naive Bayes, Categorical Naive Bayes, and others. Essentially, the presumed distribution of the feature variables can be leveraged to give an appropriate likelihood function, which aids in estimating parameters.\n\n\nFor the purposes of our analysis, Naive Bayes can be used to analyze both record and text data. Firstly, the record data we will be using is the Census data which contains survey responses on various demographic features. The purpose of this will be to apply the Categorical Naive Bayes algorithm to features such as sex, race, and marital status to fit a model with a respondant’s method of transportation to commute to work as the output variable. This will allow us to gain insights on which types of people are most affected by changes in public transit policy and service, providing valuable context for how a city’s conditions can impact its residents.\nSecondly, the labeled text data that we will use is the combined dataset of Yelp reviews for WMATA and BART. These revies contain textual responses with accompanying numerical ratings (1-5 stars). Thus, the text will be considered the feature set for Multinomial Naive Bayes, with the ratings deemed the output, as it labels the review with a scale of satisfaction or dissatisfaction with a service. The purpose of this is to analyze public sentiment regarding public transit systems and how that may affect usage or responses to potential policy changes."
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#introduction-to-naive-bayes",
    "href": "naive_bayes/naive_bayes.html#introduction-to-naive-bayes",
    "title": "Naive Bayes",
    "section": "",
    "text": "Naive Bayes classification is a group of supervised learning algorithms which use Bayes’ theorem as their foundation. The “naive” aspect of these algorithms is the assumption that every pair of features are conditionally independent. 1 This can be better understood via the Bayes’ Theorem formula and how it applies in Naive Bayes. Given an output variable \\(y\\) and several feature variables \\(x_i\\), Bayes’ Theorem posits the following:\n\\(P\\left(y | x_1, \\ldots, x_n\\right)=\\frac{P(y) P\\left(x_1, \\ldots, x_n | y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\)\nThe assumption that all feature variables are conditionally independent can be represented with the formula\n\\(P\\left(x_i \\mid y, x_1, \\ldots, x_{i-1}, x_{i+1}, \\ldots, x_n\\right)=P\\left(x_i \\mid y\\right)\\)\nwhich essentially states that the values of each \\(x_i\\) depend solely on \\(y\\), not the values from any other feature variable. Thus, we can substitute the products of all conditional probabilities into Bayes’ Theorem to represent the relationship as\n\\(P\\left(y \\mid x_1, \\ldots, x_n\\right)=\\frac{P(y) \\prod_{i=1}^n P\\left(x_i \\mid y\\right)}{P\\left(x_1, \\ldots, x_n\\right)}\\).\nUsing this foundation, Naive Bayes classification algorithms attempt to establish classifiers for a dataset by considering values for each feature and assigning probabilities that it belongs in a certain class of the specified output variable. To do this, the algorithm will first take in data from a test dataset. When executing Naive Bayes classification, it is generally recommended to partition cleaned data such that 80% of it belongs in the test dataset. The algorithm will iterate over the feature variables and observe the output, ultimately creating a model for the extents to which the feature variables predict the output classifications.\nDue to the foundation being on Bayes’ Theorem, these methods are probabilistic in nature. That is, rather than being able to decisively classify new data, the algorithms work based on conditional probabilities that records belong to a particular class.\nDiscerning when to use the different variants of Naive Bayes classification is crucial for obtaining meaningful results. Firstly, Gaussian Naive Bayes (GNB) assumes that the feature variables follow a normal distribution. Thus, if we have data that we know comes from a normally distributed population, we can use the appropriate likelihood function for parameter estimation. The same applies for other methods of Naive Bayes classification. Other popular algorithms include Multinomial Naive Bayes, Bernoulli Naive Bayes, Categorical Naive Bayes, and others. Essentially, the presumed distribution of the feature variables can be leveraged to give an appropriate likelihood function, which aids in estimating parameters.\n\n\nFor the purposes of our analysis, Naive Bayes can be used to analyze both record and text data. Firstly, the record data we will be using is the Census data which contains survey responses on various demographic features. The purpose of this will be to apply the Categorical Naive Bayes algorithm to features such as sex, race, and marital status to fit a model with a respondant’s method of transportation to commute to work as the output variable. This will allow us to gain insights on which types of people are most affected by changes in public transit policy and service, providing valuable context for how a city’s conditions can impact its residents.\nSecondly, the labeled text data that we will use is the combined dataset of Yelp reviews for WMATA and BART. These revies contain textual responses with accompanying numerical ratings (1-5 stars). Thus, the text will be considered the feature set for Multinomial Naive Bayes, with the ratings deemed the output, as it labels the review with a scale of satisfaction or dissatisfaction with a service. The purpose of this is to analyze public sentiment regarding public transit systems and how that may affect usage or responses to potential policy changes."
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "href": "naive_bayes/naive_bayes.html#preparing-data-for-naive-bayes",
    "title": "Naive Bayes",
    "section": "Preparing Data for Naive Bayes",
    "text": "Preparing Data for Naive Bayes\n\nPreparing Record Data\nTo prepare this data, the first step is to filter out columns that will not be fit in the Categorical Naive Bayes algorithm.\n\n\nCode\nfrom codecs import ignore_errors\nimport comm\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\nimport shutil\n\ncommute=pd.read_csv(\"../data/cleaned_data/commute_by_demographic.csv\")\ncommute = commute.drop(columns=['Unnamed: 0','year','id','age','personal_income','metropolitan_status'])\n\n\nThe goal of Naive Bayes is to predictions on your test data using a model that was built using test data. Thus, separating our present data into training and testing datasets is crucial for proper execution. It is generally best practice to partition 80% of this into the test dataset, which is done by specifying test_size in the train_test_split function of sklearn.\nGiven our present data, in order to execute Categorical Naive Bayes, we must encode the categorical data with associated numbers. sklearn, a machine learning software tool in Python which will be used throughout this process, has functions OrdinalEncoder and LabelEncoder to accomplish this. Thus, that will be applied to both train and test datasets once partitioned.\nNote: Code for feature selection with categorical variables partially repurposed from: https://machinelearningmastery.com/feature-selection-with-categorical-data/\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom matplotlib import pyplot\n\ndataset = commute.values\nX = dataset[:, :-1]\ny = dataset[:,-1]\nX = X.astype(str)\n\ndef prepare_inputs(X_train, X_test):\n     oe = OrdinalEncoder()\n     oe.fit(X_train)\n     X_train_enc = oe.transform(X_train)\n     X_test_enc = oe.transform(X_test)\n     return X_train_enc, X_test_enc\n\ndef prepare_targets(y_train, y_test):\n     le = LabelEncoder()\n     le.fit(y_train)\n     y_train_enc = le.transform(y_train)\n     y_test_enc = le.transform(y_test)\n     return y_train_enc, y_test_enc\n \ndef select_features(X_train, y_train, X_test):\n     fs = SelectKBest(score_func=chi2, k='all')\n     fs.fit(X_train, y_train)\n     X_train_fs = fs.transform(X_train)\n     X_test_fs = fs.transform(X_test)\n     return X_train_fs, X_test_fs, fs\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train_enc, X_test_enc = prepare_inputs(X_train, X_test)\ny_train_enc, y_test_enc = prepare_targets(y_train, y_test)\n\n\n\n\nPreparing Text Data\nTo prepare this data, we must first combine WMATA and BART datasets of Yelp reviews. Since this is not an exercise in comparing and contrasting the two, it is necessary to leverage the scalability of Naive Bayes classification by consolidating this data. The next steps are to vectorize the text, randomly shuffle the records, and again split into training and test data sets, where 80% of the data belongs to the training set.\nNote: The code for this section is partially repurposed from DSAN-5000 Lab 3.2\n\n\nCode\nwmata_yelp=pd.read_csv(\"../data/cleaned_data/wmata_reviews_cleaned.csv\")\nbart_yelp=pd.read_csv(\"../data/cleaned_data/bart_reviews_cleaned.csv\")\ntotal_yelp = pd.concat([wmata_yelp,bart_yelp])\nprint(total_yelp.shape)\nprint(total_yelp.columns)\n\n\n(1120, 3)\nIndex(['Rating', 'Date', 'Review'], dtype='object')\n\n\n\n\nCode\nreviews=[]\nratings=total_yelp['Rating']\nfor i in range(0,wmata_yelp.shape[0]):\n    keep=\"abcdefghijklmnopqrstuvwxyz \"\n    replace=\".,!;\"\n    tmp=\"\"\n    for char in wmata_yelp['Review'][i].replace(\"&lt;br /&gt;\",\"\").lower():\n        if char in replace:\n            tmp+=\" \"\n        if char in keep:\n            tmp+=char\n    tmp=\" \".join(tmp.split())\n    reviews.append(tmp)\n\nratings = np.array(ratings)\n\n\n\n\nCode\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef vectorize(corpus,MAX_FEATURES):\n    vectorizer=CountVectorizer(max_features=MAX_FEATURES,stop_words=\"english\")   \n\n    Xs  =  vectorizer.fit_transform(corpus)   \n    X=np.array(Xs.todense())\n\n    maxs=np.max(X,axis=0)\n    return (np.ceil(X/maxs),vectorizer.vocabulary_)\n\n(x,vocab0)=vectorize(reviews,MAX_FEATURES=10000)\nvocab1 = dict([(value, key) for key, value in vocab0.items()])\ndf2=pd.DataFrame(x)\ns = df2.sum(axis=0)\ndf2=df2[s.sort_values(ascending=False).index[:]]\n#print(df2.head())\ni1=0\nvocab2={}\nfor i2 in list(df2.columns):\n    vocab2[i1]=vocab1[int(i2)]\n    i1+=1\ndf2.columns = range(df2.columns.size)\nx=df2.to_numpy()\n\n\n\n\n\nCode\nimport random\nN=x.shape[0]\nl = [*range(N)]\ncut = int(0.8 * N)\nrandom.shuffle(l)\ntrain_index = l[:cut]\ntest_index = l[cut:]\n\nprint(train_index[0:10])\nprint(test_index[0:10])\n\n\n[82, 103, 95, 99, 63, 15, 93, 53, 43, 33]\n[69, 106, 6, 81, 105, 27, 9, 40, 13, 55]"
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#feature-selection",
    "href": "naive_bayes/naive_bayes.html#feature-selection",
    "title": "Naive Bayes",
    "section": "Feature Selection",
    "text": "Feature Selection\n\nFeature Selection for Record Data\nThe following code and output provides the feature selection process for our record data.\n\n\nCode\nX_train_fs, X_test_fs, fs = select_features(X_train_enc, y_train_enc, X_test_enc)\nfor i in range(len(fs.scores_)):\n     print(str(commute.columns[i]) + \": \" + str(fs.scores_[i]))\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()\n\n\nsex: 920.6032333593723\nmarital_status: 1787.933741159745\nrace: 9440.200749960755\nhispanic: 306.10785407861846\nemployment: nan\n\n\n\n\n\n\n\nFeature Selection for Text Data\nThe following code and output provides the feature selection process for our text data.\n\n\nCode\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport time\n\ndef train_MNB_model(X,Y,i_print=False):\n\n    if(i_print):\n        print(X.shape,Y.shape)\n\n    #SPLIT\n    x_train=X[train_index]\n    y_train=Y[train_index].flatten()\n\n    x_test=X[test_index]\n    y_test=Y[test_index].flatten()\n\n    # INITIALIZE MODEL \n    model = MultinomialNB()\n\n    # TRAIN MODEL \n    start = time.process_time()\n    model.fit(x_train,y_train)\n    time_train=time.process_time() - start\n\n    # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n    start = time.process_time()\n    yp_train = model.predict(x_train)\n    yp_test = model.predict(x_test)\n    time_eval=time.process_time() - start\n\n    acc_train= accuracy_score(y_train, yp_train)*100\n    acc_test= accuracy_score(y_test, yp_test)*100\n\n    if(i_print):\n        print(acc_train,acc_test,time_train,time_eval)\n\n    return (acc_train,acc_test,time_train,time_eval)\n\n\n#TEST\nprint(type(x),type(ratings))\nprint(x.shape,ratings.shape)\n(acc_train,acc_test,time_train,time_eval)=train_MNB_model(x,ratings,i_print=True)\n\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'numpy.ndarray'&gt;\n(110, 2321) (1120,)\n(110, 2321) (1120,)\n98.86363636363636 54.54545454545454 0.04070899999999966 0.01037700000000008\n\n\n\n\nCode\ndef initialize_arrays():\n    global num_features,train_accuracies\n    global test_accuracies,train_time,eval_time\n    num_features=[]\n    train_accuracies=[]\n    test_accuracies=[]\n    train_time=[]\n    eval_time=[]\n\ninitialize_arrays()\n\n# DEFINE SEARCH FUNCTION\ndef partial_grid_search(num_runs, min_index, max_index):\n    for i in range(1, num_runs+1):\n        # SUBSET FEATURES \n        upper_index=min_index+i*int((max_index-min_index)/num_runs)\n        xtmp=x[:,0:upper_index]\n\n        #TRAIN \n        (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,ratings,i_print=False)\n\n        if(i%5==0):\n            print(i,upper_index,xtmp.shape[1],acc_train,acc_test)\n            \n        #RECORD \n        num_features.append(xtmp.shape[1])\n        train_accuracies.append(acc_train)\n        test_accuracies.append(acc_test)\n        train_time.append(time_train)\n        eval_time.append(time_eval)\n\n# DENSE SEARCH (SMALL NUMBER OF FEATURES (FAST))\npartial_grid_search(num_runs=100, min_index=0, max_index=1000)\n\n# SPARSE SEARCH (LARGE NUMBER OF FEATURES (SLOWER))\npartial_grid_search(num_runs=20, min_index=1000, max_index=10000)\n\n\n5 50 50 81.81818181818183 81.81818181818183\n10 100 100 85.22727272727273 77.27272727272727\n15 150 150 89.77272727272727 68.18181818181817\n20 200 200 90.9090909090909 68.18181818181817\n25 250 250 93.18181818181817 63.63636363636363\n30 300 300 95.45454545454545 59.09090909090909\n35 350 350 96.5909090909091 50.0\n40 400 400 96.5909090909091 50.0\n45 450 450 96.5909090909091 40.909090909090914\n50 500 500 95.45454545454545 45.45454545454545\n55 550 550 95.45454545454545 50.0\n60 600 600 97.72727272727273 50.0\n65 650 650 97.72727272727273 54.54545454545454\n70 700 700 97.72727272727273 50.0\n75 750 750 97.72727272727273 59.09090909090909\n80 800 800 97.72727272727273 50.0\n85 850 850 97.72727272727273 50.0\n90 900 900 97.72727272727273 45.45454545454545\n95 950 950 97.72727272727273 45.45454545454545\n100 1000 1000 97.72727272727273 54.54545454545454\n5 3250 2321 98.86363636363636 54.54545454545454\n10 5500 2321 98.86363636363636 54.54545454545454\n15 7750 2321 98.86363636363636 54.54545454545454\n20 10000 2321 98.86363636363636 54.54545454545454\n\n\n\n\nCode\ndef save_results(path_root):\n    out=np.transpose(np.array([num_features,train_accuracies,test_accuracies,train_time,eval_time])) \n    out=pd.DataFrame(out)\n    out.to_csv(path_root+\".csv\")\n\ndef plot_results(path_root):\n    plt.plot(num_features,train_accuracies,'-or')\n    plt.plot(num_features,test_accuracies,'-ob')\n    plt.xlabel('Number of features')\n    plt.ylabel('ACCURACY: Training (blue) and Test (red)')\n    plt.savefig(path_root+'-1.png')\n    plt.show()\n\n    plt.plot(num_features,np.array(train_accuracies)-np.array(test_accuracies),'-or')\n    plt.xlabel('Number of features')\n    plt.ylabel('train_accuracies-test_accuracies')\n    plt.savefig(path_root+'-4.png')\n    plt.show()\n\nsave_results('../data/naive_bayes'+\"/partial_grid_search\")\nplot_results('../data/naive_bayes'+\"/partial_grid_search\")\n\n\n\n\n\n\n\n\n\n\nCode\nx_var=np.var(x,axis=0)\nfrom sklearn.feature_selection import VarianceThreshold\n\n# DEFINE GRID OF THRESHOLDS \nnum_thresholds=30\nthresholds=np.linspace(np.min(x_var),np.max(x_var),num_thresholds)\n\n#DOESN\"T WORK WELL WITH EDGE VALUES \nthresholds=thresholds[1:-2]; #print(thresholds)\n\n# INITIALIZE ARRAYS\ninitialize_arrays()\n\n# SEARCH FOR OPTIMAL THRESHOLD\nfor THRESHOLD in thresholds:\n    feature_selector = VarianceThreshold(threshold=THRESHOLD)\n    xtmp=feature_selector.fit_transform(x)\n    print(\"THRESHOLD =\",THRESHOLD, xtmp.shape[1])\n\n    (acc_train,acc_test,time_train,time_eval)=train_MNB_model(xtmp,ratings,i_print=False)\n             \n    #RECORD \n    num_features.append(xtmp.shape[1])\n    train_accuracies.append(acc_train)\n    test_accuracies.append(acc_test)\n    train_time.append(time_train)\n    eval_time.append(time_eval)\n\n\nTHRESHOLD = 0.017306925049871733 828\nTHRESHOLD = 0.025605585636933575 480\nTHRESHOLD = 0.03390424622399542 322\nTHRESHOLD = 0.04220290681105726 225\nTHRESHOLD = 0.050501567398119104 169\nTHRESHOLD = 0.05880022798518093 128\nTHRESHOLD = 0.06709888857224278 96\nTHRESHOLD = 0.07539754915930462 67\nTHRESHOLD = 0.08369620974636646 58\nTHRESHOLD = 0.0919948703334283 47\nTHRESHOLD = 0.10029353092049015 40\nTHRESHOLD = 0.10859219150755198 37\nTHRESHOLD = 0.11689085209461382 33\nTHRESHOLD = 0.12518951268167566 26\nTHRESHOLD = 0.13348817326873752 23\nTHRESHOLD = 0.14178683385579935 20\nTHRESHOLD = 0.15008549444286118 19\nTHRESHOLD = 0.15838415502992303 19\nTHRESHOLD = 0.16668281561698486 16\nTHRESHOLD = 0.17498147620404672 14\nTHRESHOLD = 0.18328013679110855 9\nTHRESHOLD = 0.1915787973781704 7\nTHRESHOLD = 0.19987745796523224 5\nTHRESHOLD = 0.20817611855229406 4\nTHRESHOLD = 0.21647477913935592 4\nTHRESHOLD = 0.22477343972641775 2\nTHRESHOLD = 0.2330721003134796 1\n\n\n\n\nCode\nsave_results(\"../data/naive_bayes\"+\"/variance_threshold\")\nplot_results(\"../data/naive_bayes\"+\"/variance_threshold\")"
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#naive-bayes",
    "href": "naive_bayes/naive_bayes.html#naive-bayes",
    "title": "Naive Bayes",
    "section": "Naive Bayes",
    "text": "Naive Bayes\n\nNaive Bayes with Labeled Record Data\nThe following code, output, and comments show the process for Naive Bayes classification on record data.\n\n\nCode\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.naive_bayes import CategoricalNB\nimport scipy\nfrom scipy import stats\nclf = CategoricalNB(force_alpha=True)\nenc = OrdinalEncoder()\n\nfeature_cols = feature_cols = [c for c in commute.columns if c != 'transportation_type']\nX_train_df = pd.DataFrame(X_train_enc, columns=feature_cols)\nX_test_df = pd.DataFrame(X_test_enc, columns=feature_cols)\nincluded_vars = ['sex','race','marital_status']\nincluded_vars_df = X_train_df[included_vars].copy()\nsex_marital_corr = stats.spearmanr(included_vars_df['sex'], included_vars_df['marital_status']).statistic\nprint(sex_marital_corr)\nsex_race_corr = stats.spearmanr(included_vars_df['sex'], included_vars_df['race']).statistic\nprint(sex_race_corr)\nrace_marital_corr = stats.spearmanr(included_vars_df['race'], included_vars_df['marital_status']).statistic\nprint(race_marital_corr)\nsex_commute_corr = stats.spearmanr(included_vars_df['sex'], y_train).statistic\nprint(sex_commute_corr)\nrace_commute_corr = stats.spearmanr(included_vars_df['race'], y_train).statistic\nprint(race_commute_corr)\nmarital_commute_corr = stats.spearmanr(included_vars_df['marital_status'], y_train).statistic\nprint(marital_commute_corr)\n\n\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n0.004576510748920096\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n0.02050209664803721\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.09405440809270636\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.04048269718986366\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.005574625067789635\nIntel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n-0.005549827373300944\n\n\n\n\nCode\nk = 3\nmean_xx_corr = np.mean([sex_marital_corr,sex_race_corr,race_marital_corr])\nmean_xy_corr = np.mean([sex_commute_corr,race_commute_corr,marital_commute_corr])\nprint(f\"Number of features: {k}\")\nmerit_score_numer = k * np.absolute(mean_xy_corr)\nmerit_score_denom = np.sqrt(k + k * (k + 1) * np.absolute(mean_xx_corr))\nmerit_score_s2 = merit_score_numer / merit_score_denom\nprint(f\"Merit score: {merit_score_s2}\")\n\n\nNumber of features: 3\nMerit score: 0.02851309377702586\n\n\n\n\nCode\nenc.fit(X_train)\nX_train_enc = enc.transform(X_train)\nclf.fit(X_train_enc, y_train)\n\n\n\n\nCode\nfrom sklearn.metrics import f1_score\nenc.fit(X_test)\nX_test_enc = enc.transform(X_test)\ntest_predictions = clf.predict(X_test_enc)\nf1_score(y_true = y_test, y_pred = test_predictions, average='weighted')\n\n\n0.6442288060648733\n\n\nThe following F1 score combines the precision and recall of a classifier. Given this score, it appears that our model was somewhat effective at predicting commute methods in terms of these metrics.\n\n\nNaive Bayes with Labeled Text Data\nThe following code, output, and comments show the process for Naive Bayes classification on record data.\n\n\nCode\nwmata_yelp.head()\n\n\n\n\n\n\n\n\n\nRating\nDate\nReview\n\n\n\n\n0\n5\nJul 26, 2023\nI had to compliment WMATA on the shuttle servi...\n\n\n1\n1\nOct 27, 2023\nSince when did metro close the doors to their ...\n\n\n2\n1\nSep 29, 2023\nSo many things wrong with wmata I can't even b...\n\n\n3\n5\nSep 23, 2023\nI WFH, and WMATA has been very helpful in my m...\n\n\n4\n4\nSep 18, 2022\nI took the metro while visiting DC. I began at...\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import MultinomialNB\n\n# CONVERT DF TO LIST OF STRINGS \ncorpus=total_yelp[\"Review\"].to_list()\ny2=total_yelp[\"Rating\"].to_numpy()\n\nprint(\"number of text chunks = \",len(corpus))\nprint(corpus[0:3])\n\n\nnumber of text chunks =  1120\n[\"I had to compliment WMATA on the shuttle service from College Park to Ft.Totten. I have been greeted with so many warm smiles and friendly directions and guidance to the shuttles!!!! Ty All! Hey, and honorable mention on the increase in security on and around the trains!!! In the past I have been harassed on 3 different occasions, and was seriously thinking of driving to work. As of recently, I'm quite comfortable knowing security is visible and obtainable quickly. Keep up the improvements and great work!!! Thank you!\", 'Since when did metro close the doors to their under serviced lines before the group of people WAITING to get in could even get in. That is unimaginably frustrating. Blue line to Franconia Metro center at 2:56 pm today.', \"So many things wrong with wmata I can't even begin to list them all.  For one, this morning toda a train derailed.  Maintenance seems to be the #1 issue, followed by people jumping fare gates. It wouldn't seem so bad if it happens once in a while, but to see it happening every since time I go through fare gates as I'm coming and going is a little much by people who don't care who else they affect\"]\n\n\n\n\nCode\nvectorizer=CountVectorizer(min_df=0.001)   \n\n# RUN COUNT VECTORIZER ON OUR COURPUS \ntext_Xs  =  vectorizer.fit_transform(corpus)   \ntext_X=np.array(text_Xs.todense())\n\n#CONVERT TO ONE-HOT VECTORS\nmaxs=np.max(text_X,axis=0)\ntext_X=np.ceil(text_X/maxs)\n\n# DOUBLE CHECK \nprint(text_X.shape,y2.shape)\nprint(\"DATA POINT-0:\",text_X[0,0:10],\"  y2 =\",y2[0])\n\n\n(1120, 1530) (1120,)\nDATA POINT-0: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   y2 = 5\n\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\n\ntest_ratio=0.2\n\n# SPLIT ARRAYS OR MATRICES INTO RANDOM TRAIN AND TEST SUBSETS.\ntext_x_train, text_x_test, text_y_train, text_y_test = train_test_split(text_X, y2, test_size=test_ratio, random_state=0)\ntext_y_train=text_y_train.flatten()\ntext_y_test=text_y_test.flatten()\n\nprint(\"x_train.shape        :\",text_x_train.shape)\nprint(\"y_train.shape        :\",text_y_train.shape)\n\nprint(\"X_test.shape     :\",text_x_test.shape)\nprint(\"y_test.shape     :\",text_y_test.shape)\nprint(y_train[0:100])\n\n\nx_train.shape       : (896, 1530)\ny_train.shape       : (896,)\nX_test.shape        : (224, 1530)\ny_test.shape        : (224,)\n['Work_from_home' 'Private_vehicle' 'Work_from_home' 'Private_vehicle'\n 'Work_from_home' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Work_from_home' 'Work_from_home' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Work_from_home' 'Private_vehicle' 'Work_from_home'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Work_from_home' 'Private_vehicle'\n 'Private_vehicle' 'Work_from_home' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Work_from_home'\n 'Private_vehicle' 'Private_vehicle' 'Work_from_home' 'Private_vehicle'\n 'Private_vehicle' 'Work_from_home' 'Private_vehicle' 'Private_vehicle'\n 'Work_from_home' 'Private_vehicle' 'Work_from_home' 'Work_from_home'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Public_transit' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Work_from_home'\n 'Other' 'Private_vehicle' 'Work_from_home' 'Work_from_home' 'Other'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Work_from_home' 'Private_vehicle'\n 'Private_vehicle' 'Private_vehicle' 'Private_vehicle' 'Work_from_home'\n 'Work_from_home' 'Private_vehicle' 'Private_vehicle' 'Private_vehicle'\n 'Work_from_home' 'Private_vehicle' 'Private_vehicle']\n\n\n\n\nCode\nfrom sklearn import model_selection\n\ndef report(y,ypred):\n      #ACCURACY COMPUTE \n      print(\"Accuracy:\",accuracy_score(y, ypred)*100)\n      print(\"Number of mislabeled points out of a total %d points = %d\"\n            % (y.shape[0], (y != ypred).sum()))\n\ndef print_model_summary(model):\n      # LABEL PREDICTIONS FOR TRAINING AND TEST SET \n      text_yp_train = model.predict(text_x_train)\n      text_yp_test = model.predict(text_x_test)\n\n      print(\"ACCURACY CALCULATION\\n\")\n\n      print(\"TRAINING SET:\")\n      report(text_y_train,text_yp_train)\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      report(text_y_test,text_yp_test)\n\n      print(\"\\nCHECK FIRST 20 PREDICTIONS\")\n      print(\"TRAINING SET:\")\n      print(text_y_train[0:20])\n      print(text_yp_train[0:20])\n      print(\"ERRORS:\",text_yp_train[0:20]-text_y_train[0:20])\n\n      print(\"\\nTEST SET (UNTRAINED DATA):\")\n      print(text_y_test[0:20])\n      print(text_yp_test[0:20])\n      print(\"ERRORS:\",text_yp_test[0:20]-text_y_test[0:20])\n\n\n\n\nCode\nmodel = MultinomialNB()\nmodel.fit(text_x_train,text_y_train)\nprint_model_summary(model)\n\n\nACCURACY CALCULATION\n\nTRAINING SET:\nAccuracy: 91.85267857142857\nNumber of mislabeled points out of a total 896 points = 73\n\nTEST SET (UNTRAINED DATA):\nAccuracy: 91.51785714285714\nNumber of mislabeled points out of a total 224 points = 19\n\nCHECK FIRST 20 PREDICTIONS\nTRAINING SET:\n[1 1 2 1 1 3 3 1 1 3 1 1 3 1 1 3 1 1 1 1]\n[1 1 2 1 1 3 3 1 1 3 1 1 3 1 1 3 1 1 5 1]\nERRORS: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0]\n\nTEST SET (UNTRAINED DATA):\n[1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1 3 1 1 3]\n[1 1 1 1 1 1 1 3 1 1 1 1 3 1 1 1 3 1 1 3]\nERRORS: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n\n\nNote: Conclusions and visualizations not yet complete."
  },
  {
    "objectID": "naive_bayes/naive_bayes.html#footnotes",
    "href": "naive_bayes/naive_bayes.html#footnotes",
    "title": "Naive Bayes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“1.9. naive Bayes,” scikit, https://scikit-learn.org/stable/modules/naive_bayes.html (accessed Nov. 2, 2023).↩︎"
  },
  {
    "objectID": "arm/arm.html",
    "href": "arm/arm.html",
    "title": "ARM",
    "section": "",
    "text": "Build out your website tab for “ARM”"
  },
  {
    "objectID": "cleaning/cleaning.html",
    "href": "cleaning/cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "To see all cleaned data click here"
  },
  {
    "objectID": "cleaning/cleaning.html#footnotes",
    "href": "cleaning/cleaning.html#footnotes",
    "title": "Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Ridership Report.” American Public Transportation Association, 21 Sept. 2023, www.apta.com/research-technical-resources/transit-statistics/ridership-report/.↩︎\n“News API – Search News and Blog Articles on the Web.” News API Â Search News and Blog Articles on the Web, newsapi.org/. Accessed 12 Oct. 2023.↩︎\nBarrero, Jose Maria, et al. Why Working from Home Will Stick, 2021, https://doi.org/10.3386/w28731.↩︎\n“Washington Metropolitan Area Transit Authority.” WMATA, www.wmata.com/initiatives/ridership-portal/. Accessed 12 Oct. 2023.↩︎\n“Ridership Reports.” Ridership Reports | Bay Area Rapid Transit, www.bart.gov/about/reports/ridership. Accessed 13 Oct. 2023.↩︎\nU.S. Census Bureau. “MEANS OF TRANSPORTATION TO WORK BY SELECTED CHARACTERISTICS.” American Community Survey, ACS 5-Year Estimates Subject Tables, Table S0802, 2021, https://data.census.gov/table/ACSST5Y2021.S0802?t=Commuting&g=860XX00US20020,20032. Accessed on October 12, 2023.↩︎\nSteven Ruggles, Sarah Flood, Matthew Sobek, Danika Brockman, Grace Cooper, Stephanie Richards, and Megan Schouweiler. IPUMS USA: Version 13.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D010.V13.0↩︎\n“WMATA - Washington, DC, DC,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 2, 2023).↩︎\n“Bart - Bay Area Rapid Transit - Oakland, CA,” Yelp, https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2 (accessed Nov. 2, 2023).↩︎"
  },
  {
    "objectID": "data/data.html",
    "href": "data/data.html",
    "title": "Data",
    "section": "",
    "text": "Link to Github repository"
  },
  {
    "objectID": "data/data.html#footnotes",
    "href": "data/data.html#footnotes",
    "title": "Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Quarterly and Annual Totals by Mode​​​,” American Public Transportation Association, https://www.apta.com/research-technical-resources/transit-statistics/ridership-report/ (accessed Oct. 5, 2023).↩︎"
  },
  {
    "objectID": "gathering/gathering.html",
    "href": "gathering/gathering.html",
    "title": "Data Gathering",
    "section": "",
    "text": "To see all raw data gathered click here"
  },
  {
    "objectID": "gathering/gathering.html#footnotes",
    "href": "gathering/gathering.html#footnotes",
    "title": "Data Gathering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Ridership Report.” American Public Transportation Association, 21 Sept. 2023, www.apta.com/research-technical-resources/transit-statistics/ridership-report/.↩︎\n“News API – Search News and Blog Articles on the Web.” News API Â Search News and Blog Articles on the Web, newsapi.org/. Accessed 12 Oct. 2023.↩︎\nBarrero, Jose Maria, et al. Why Working from Home Will Stick, 2021, https://doi.org/10.3386/w28731.↩︎\n“Washington Metropolitan Area Transit Authority.” WMATA, www.wmata.com/initiatives/ridership-portal/. Accessed 12 Oct. 2023.↩︎\n“Ridership Reports.” Ridership Reports | Bay Area Rapid Transit, www.bart.gov/about/reports/ridership. Accessed 13 Oct. 2023.↩︎\nU.S. Census Bureau. “MEANS OF TRANSPORTATION TO WORK BY SELECTED CHARACTERISTICS.” American Community Survey, ACS 5-Year Estimates Subject Tables, Table S0802, 2021, https://data.census.gov/table/ACSST5Y2021.S0802?t=Commuting&g=860XX00US20020,20032. Accessed on October 12, 2023.↩︎\nSteven Ruggles, Sarah Flood, Matthew Sobek, Danika Brockman, Grace Cooper, Stephanie Richards, and Megan Schouweiler. IPUMS USA: Version 13.0 [dataset]. Minneapolis, MN: IPUMS, 2023. https://doi.org/10.18128/D010.V13.0↩︎\n“WMATA - Washington, DC, DC,” Yelp, https://www.yelp.com/biz/wmata-washington (accessed Nov. 2, 2023).↩︎\n“Bart - Bay Area Rapid Transit - Oakland, CA,” Yelp, https://www.yelp.com/biz/bart-bay-area-rapid-transit-oakland-2 (accessed Nov. 2, 2023).↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About the Author",
    "section": "",
    "text": "Lab 1.1.3\n\n\n\n\n\n\nJosh Sweren is a graduate student in the Data Science and Analytics program at Georgetown University. He received his B.S. in Statistics with minors in Computer Science and Mathematics from the George Washington University in 2020. Since graduating, he has remained in the Washington, D.C. area, and has worked in government consulting for over three years. He currently works as a Business Analyst for an IT contractor, serving under a contract with the United States Department of Agriculture. Originally from New York, Mr. Sweren is eager to learn about Machine Learning and Deep Learning, as well as the broader topics of data visualization and communication. Outside of work, he enjoys playing and watching sports, as well as doing puzzles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Coursework\n\n\n\nDSAN 5000: Data Science and Analytics\n\n\n\nDSAN 5100: Probabilistic Modeling and Statistical Computing\n\n\n\n\nRelevant Prior Coursework\n\n\n\nBig Data & Analytics\n\n\n\nStatistical Computer Packages\n\n\n\nMathematical Statistics\n\n\n\nCombinatorics\n\n\n\nBayesian Statistics"
  }
]