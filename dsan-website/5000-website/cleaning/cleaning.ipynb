{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Cleaning\"\n",
    "format:\n",
    "  html:\n",
    "      embed-resources: true\n",
    "      code-fold: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all cleaned data [click here](https://github.com/anly501/dsan-5000-project-jsweren1/tree/main/dsan-website/5000-website/data/cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quarterly and Annual Ridership Totals by Mode​ of Transportation\n",
    "\n",
    "The purpose of this data is to gain a baseline perspective of the current state of public transit usage in the United States. Therefore, this data set should be cleaned in a way that trends can be visualized, without including superfluous information that does not relate to any current phenomena. The steps used in cleaning this data are below.\n",
    "\n",
    "- Trim the data set:\n",
    "  - Columns 1 to 11 to trim blank items in the .csv file, as well as notes put in by the source.\n",
    "  - Rows 81 to 133 to remove records from prior to 2010, as those are superfluous when comparing to current trends.\n",
    "- Create one column to account for year and quarter to improve readability\n",
    "- Convert all numeric rows to numeric data type\n",
    "- Remove extra year and quarter columns as they are now unnecessary\n",
    "\n",
    "Regarding the numeric fields, I have chosen to keep them all for now as each one can provide insight into which modes of transportation are most affected by certain factors. Below is the code to apply the steps laid out, as well as a comparison between the raw and cleaned data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "ridership <- read.csv(\"../data/APTA-Ridership-by-Mode-and-Quarter-1990-Present.csv\")\n",
    "ridership <- ridership[81:133,1:11]\n",
    "colnames(ridership)[2] <- 'Year - Quarter'\n",
    "colnames(ridership)[4:11] <- c(\"total_ridership\", \"heavy_rail\", \"light_rail\", \"commuter_rail\", \"trolleybus\", \"bus\", \"demand_response\", \"other\")\n",
    "ridership$total_ridership <- as.numeric(gsub(\",\",\"\", ridership$total_ridership))\n",
    "ridership$heavy_rail <- as.numeric(gsub(\",\",\"\", ridership$heavy_rail))\n",
    "ridership$light_rail <- as.numeric(gsub(\",\",\"\", ridership$light_rail))\n",
    "ridership$commuter_rail <- as.numeric(gsub(\",\",\"\", ridership$commuter_rail))\n",
    "ridership$trolleybus <- as.numeric(gsub(\",\",\"\", ridership$trolleybus))\n",
    "ridership$bus <- as.numeric(gsub(\",\",\"\", ridership$bus))\n",
    "ridership$demand_response <- as.numeric(gsub(\",\",\"\", ridership$demand_response))\n",
    "ridership$other <- as.numeric(gsub(\",\",\"\", ridership$other))\n",
    "ggplot(data=ridership, aes(x=factor(`Year - Quarter`), y=total_ridership, group=1, xmin = \"2015 - Q1\", xmax=\"2023-Q1\")) +\n",
    "  geom_line()+\n",
    "  geom_point()+\n",
    "  labs(x = \"Year - Quarter\", y = \"Total Ridership (000s)\", title = \"Total Public Transit Ridership in the U.S.\")+\n",
    "  theme(axis.text.x = element_text(angle = 45))\n",
    "ridership <- ridership[c(2, 4:11)]\n",
    "head(ridership)\n",
    "write.csv(ridership, \"../data/cleaned_data/ridership_by_quarter_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Raw Quarterly Ridership Data](../images/apta_raw_data.png)\n",
    "\n",
    "![Cleaned Quarterly Ridership Data](../images/quarterly_ridership_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News API Data\n",
    "\n",
    "This data in its raw form comes as a JSON file with each record corresponding to a particular article. The purpose of cleaning this will be to analyze word prevalence, which can be done by creating a corpus. The steps for this are recycled from DSAN-5000 Lab 2.1, and are described as follows:\n",
    "\n",
    "- Retrieve the raw data JSON file for WMATA news.\n",
    "- Create a string cleaning function to deal with punctuation, special characters, and differently cased letters\n",
    "- Iterate through each article\n",
    "  - Iterate through each data point in an article to clean strings and append cleaned data to output list\n",
    "- Convert cleaned data to data frame\n",
    "- Create corpus from cleaned data\n",
    "- Use `CountVectorizer` to retrieve vocabulary for the data set\n",
    "- Repeat for BART\n",
    "\n",
    "Below is the code, along with images of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "total_requests=2\n",
    "verbose=True\n",
    "\n",
    "#WMATA\n",
    "\n",
    "x = open('../data/WMATA-newapi-raw-data.json')\n",
    "response = json.load(x)\n",
    "\n",
    "def string_cleaner(input_string):\n",
    "    try: \n",
    "        out=re.sub(r\"\"\"\n",
    "                    [,.;@#?!&$-]+\n",
    "                    \\ *\n",
    "                    \"\"\",\n",
    "                    \" \",\n",
    "                    input_string, flags=re.VERBOSE)\n",
    "\n",
    "        out = re.sub('[’.]+', '', input_string)\n",
    "        out = re.sub(r'\\s+', ' ', out)\n",
    "        out=out.lower()\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        out=''\n",
    "    return out\n",
    "\n",
    "article_list=response['articles']\n",
    "article_keys=article_list[0].keys()\n",
    "index=0\n",
    "cleaned_data=[];  \n",
    "for article in article_list:\n",
    "    tmp=[]\n",
    "    for key in article_keys:\n",
    "        if(key=='source'):\n",
    "            src=string_cleaner(article[key]['name'])\n",
    "            tmp.append(src) \n",
    "\n",
    "        if(key=='author'):\n",
    "            author=string_cleaner(article[key])\n",
    "            if(src in author): \n",
    "                author='NA'\n",
    "            tmp.append(author)\n",
    "\n",
    "        if(key=='title'):\n",
    "            tmp.append(string_cleaner(article[key]))\n",
    "\n",
    "        if(key=='publishedAt'):\n",
    "            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n",
    "            date=article[key]\n",
    "            if(not ref.match(date)):\n",
    "                date=\"NA\"\n",
    "            tmp.append(date)\n",
    "\n",
    "    cleaned_data.append(tmp)\n",
    "    index+=1\n",
    "\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "print(df.head())\n",
    "df.to_csv('../data/cleaned_data/wmata_news_cleaned.csv', index=False)\n",
    "\n",
    "corpus = df[2]\n",
    "vectorizer=CountVectorizer()\n",
    "word_counts  =  vectorizer.fit_transform(corpus)\n",
    "print(\"vocabulary = \",vectorizer.vocabulary_)\n",
    "\n",
    "\n",
    "## BART\n",
    "x = open('../data/BART-newapi-raw-data.json')\n",
    "response = json.load(x)\n",
    "\n",
    "article_list=response['articles']\n",
    "article_keys=article_list[0].keys()\n",
    "index=0\n",
    "cleaned_data=[];  \n",
    "for article in article_list:\n",
    "    tmp=[]\n",
    "    for key in article_keys:\n",
    "        if(key=='source'):\n",
    "            src=string_cleaner(article[key]['name'])\n",
    "            tmp.append(src) \n",
    "\n",
    "        if(key=='author'):\n",
    "            author=string_cleaner(article[key])\n",
    "            if(src in author): \n",
    "                author='NA'\n",
    "            tmp.append(author)\n",
    "\n",
    "        if(key=='title'):\n",
    "            tmp.append(string_cleaner(article[key]))\n",
    "\n",
    "        if(key=='publishedAt'):\n",
    "            ref = re.compile('.*-.*-.*T.*:.*:.*Z')\n",
    "            date=article[key]\n",
    "            if(not ref.match(date)):\n",
    "                date=\"NA\"\n",
    "            tmp.append(date)\n",
    "\n",
    "    cleaned_data.append(tmp)\n",
    "    index+=1\n",
    "\n",
    "df = pd.DataFrame(cleaned_data)\n",
    "print(df.head())\n",
    "df.to_csv('../data/cleaned_data/bart_news_cleaned.csv', index=False)\n",
    "\n",
    "corpus = df[2]\n",
    "vectorizer=CountVectorizer()\n",
    "word_counts  =  vectorizer.fit_transform(corpus)\n",
    "print(\"vocabulary = \",vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cleaned WMATA News Data](../images/wmata_news_cleaned.png)\n",
    "\n",
    "![Cleaned BART News Data](../images/bart_news_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Work Trends - Desires of Employers vs. Workers\n",
    "\n",
    "The insight to be gathered from this data would be the discrepancies between what employers want from their workers' remote work schedule, and those of the workers themselves. Therefore, while these come from two separate .csv files, it will be necessary to merge these data sets into one data frame. Additionally, each data set has two variables: \n",
    "1. The amount of working from home (days per week) employers or workers want for all workers\n",
    "2. The amount of working from home (days per week) employers or workers want for workers able to work from home\n",
    "Since both of these have ample data, we will keep both. The methodology for this is as follows:\n",
    "\n",
    "- Read both data sets and trim excess space where the owner of the data had included a citation note\n",
    "- Merge by date\n",
    "  - These data sets come from the same series of surveys, so the date column is exactly the same, eliminating any need for removal of rows.\n",
    "- Convert the date field to a date data type and order by date\n",
    "- Rename columns based on glossary provided by the data source\n",
    "- Ensure numeric columns have numeric data type\n",
    "- Remove rows in which there are too many `NA` values.\n",
    "  - Rows in which the values for all workers **OR** workers able to work from home have `NA` values can be kept, as there is a comparison to be made with the ones that don't have `NA` values. Only rows in which no comparison can be made will be removed.\n",
    "\n",
    "The code for this is below, along with a screenshot of the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "employer <- read.csv(\"../data/WFH_monthly/WFH_monthly_employer.csv\")\n",
    "worker <- read.csv(\"../data/WFH_monthly/WFH_monthly_worker.csv\")\n",
    "employer <- employer[c(1:3)]\n",
    "worker <- worker[c(1:3)]\n",
    "plans <- merge(employer, worker, by = \"date\")\n",
    "plans$date <- as.Date(plans$date, format = \"%m/%d/%y\")\n",
    "plans <- plans[order(plans$date),]\n",
    "colnames(plans)[c(2:5)] <- c(\"employer_desires_all\", \"employer_desires_able\", \"worker_desires_all\", \"worker_desires_able\")\n",
    "typeof(plans$employer_desires_all)\n",
    "plans <- plans[!(is.na(plans$employer_desires_all) & is.na(plans$employer_desires_able)),]\n",
    "head(plans)\n",
    "write.csv(plans, \"../data/cleaned_data/WFH_surveys_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cleaned Data for Remote Work Plans of Employers and Workers](../images/wfh_plans_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Work Trends by City\n",
    "\n",
    "The cleaning methodology for this data set is simple: reduce from the top ten largest cities in the United States to the cities which we are focusing on. The steps for this, along with the code and before/after screenshots are below:\n",
    "\n",
    "- Read .csv file and remove all columns except for the date of each survey, results from Washington, D.C., and results from the San Francisco Bay Area\n",
    "- Convert the date field to a date data type and order by date\n",
    "- Ensure no `NA` values and that numeric columns have a numeric data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "city <- read.csv(\"../data/WFH_monthly/WFH_monthly_city.csv\")\n",
    "city <- city[c(1,6,8)]\n",
    "city$date <- as.Date(city$date, format = \"%m/%d/%y\")\n",
    "city <- city[order(city$date),]\n",
    "colnames(city)[c(2,3)] <- c(\"wfh_BayArea\", \"wfh_WashingtonDC\")\n",
    "city <- na.omit(city)\n",
    "typeof(city$wfh_BayArea)\n",
    "head(city)\n",
    "write.csv(city, \"../data/cleaned_data/WFH_city_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Remote Work Percentages by City - Raw](../images/wfh_city.png)\n",
    "\n",
    "![Remote Work Percentages by City - Cleaned](../images/wfh_city_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridership Trends by City\n",
    "\n",
    "These data sets will allow us to compare total and average monthly riderships between Washington, D.C. and the San Francisco Bay Area. To do this, we will need the data to match up to avoid any unintended discrepancies. Thus, the date range that has been selected is January, 2018 to September, 2023. First, the WMATA data comes with all average monthly entries listed in a single row, as shown below:\n",
    "\n",
    "![WMATA Average Daily Entries by Month](../images/wmata_monthly_boarding.png)\n",
    "\n",
    "Since the data unit we are after is each month, this should ultimately be transposed when cleaning. Additionally, we will need to combine the years, which act as column names in the raw data, with the months. The steps for this are as follows:\n",
    "\n",
    "- Read .csv file and remove final row, which is duplicative. It is simply a truncated version of the data directly above it.\n",
    "- Retrieve column names to create a list of years\n",
    "- Transpose rows containing months and values and add them to a data frame with the `years` column\n",
    "- Remove blank row created by this transposition\n",
    "- Create date column by concatenating year and month and converting it to date type\n",
    "- Re-arrange columns, remove duplicative columns containing year and month, and rename `avg_daily_entries` column\n",
    "\n",
    "The code and screenshot are shown below:\n",
    "\n",
    "**Note: BART data has not yet been cleaned.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "wmata_monthly <- read.csv(\"../data/WMATA_boardings_by_month.csv\")\n",
    "wmata_monthly <- wmata_monthly[c(1,2),]\n",
    "wmata_years <- c(colnames(wmata_monthly))\n",
    "wmata <- data.frame(wmata_years, t(wmata_monthly[1,]), t(wmata_monthly[2,]))\n",
    "wmata <- wmata[-1,]\n",
    "wmata$date <- paste(1, wmata$X1, substr(wmata$wmata_years, 2, 5))\n",
    "wmata$date <- as.Date(wmata$date, \"%d %B %Y\")\n",
    "wmata <- wmata[c(4, 3)]\n",
    "colnames(wmata)[2] <- \"avg_daily_entries\"\n",
    "head(wmata)\n",
    "write.csv(wmata, \"../data/cleaned_data/wmata_monthly_ridership.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![WMATA Average Daily Entries by Month - Cleaned](../images/wmata_monthly_cleaned.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridership by Hour\n",
    "\n",
    "These data sets can be used to compare ridership trends before and after the pandemic and all ramifications that came from it. These both show average daily entries and exits in Washington, D.C. by hour of the day, allowing us to see when people use public transit, and ultimately infer why they may be using it. The difference in the data is that the `before` data set contains data from January 1, 2018 to March 17, 2020, while the `after` data set contains data from March 18, 2020 to October 5, 2023.\n",
    "\n",
    "The steps for cleaning these data sets are as follows:\n",
    "\n",
    "- Read .csv files and remove rounded fields, as they are duplicative\n",
    "- Rename columns for readability\n",
    "- Convert numeric columns to numeric data type\n",
    "- Introduce `hour_numeric` column for future time series analysis\n",
    "- Rearrange columns\n",
    "\n",
    "The code for carrying this out and screenshots of the cleaned `before` data set are below. Additionally, plots of the data sets have been charted to visualize the data that is being obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "\n",
    "before <- read.csv(\"../data/WMATA_boardings_by_hour/boardings_pre-covid.csv\")\n",
    "after <- read.csv(\"../data/WMATA_boardings_by_hour/boardings_post-covid.csv\")\n",
    "before <- before[c(1,2,4)]\n",
    "after <- after[c(1,2,4)]\n",
    "colnames(before) <- c(\"hour\", \"avg_daily_entries\", \"avg_daily_exits\")\n",
    "colnames(after) <- c(\"hour\", \"avg_daily_entries\", \"avg_daily_exits\")\n",
    "before$avg_daily_entries <- as.numeric(gsub(\",\",\"\", before$avg_daily_entries))\n",
    "before$avg_daily_exits <- as.numeric(gsub(\",\",\"\", before$avg_daily_exits))\n",
    "after$avg_daily_entries <- as.numeric(gsub(\",\",\"\", after$avg_daily_entries))\n",
    "after$avg_daily_exits <- as.numeric(gsub(\",\",\"\", after$avg_daily_exits))\n",
    "before$hour_numeric <- c(4:23, 0:3)\n",
    "after$hour_numeric <- c(4:23, 0:3)\n",
    "before <- before[c(1,4,2,3)]\n",
    "after <- after[c(1,4,2,3)]\n",
    "ggplot(data=before, aes(x=factor(hour_numeric, ordered = FALSE), y=avg_daily_entries, group=1)) +\n",
    "  geom_line()+\n",
    "  geom_point()+\n",
    "  labs(x = \"Numeric Hour of Day\", y = \"Average Daily Entries\", title = \"Average Daily Entries by Hour (Pre-Pandemic)\")\n",
    "ggplot(data=after, aes(x=factor(hour_numeric, ordered = FALSE), y=avg_daily_entries, group=1)) +\n",
    "  geom_line()+\n",
    "  geom_point()+\n",
    "  labs(x = \"Numeric Hour of Day\", y = \"Average Daily Entries\", title = \"Average Daily Entries by Hour (Post-Pandemic)\")\n",
    "head(before)\n",
    "head(after)\n",
    "write.csv(before, \"../data/cleaned_data/hourly_average_cleaned_pre-covid.csv\")\n",
    "write.csv(after, \"../data/cleaned_data/hourly_average_cleaned_post-covid.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Hourly Ridership from 1/1/2018 to 3/17/2020 - Cleaned](../images/hourly_cleaned.png)\n",
    "\n",
    "![](../images/hourly_entries_before.png)\n",
    "\n",
    "![](../images/hourly_entries_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridership by Demographic\n",
    "\n",
    "Lastly, the purpose of this data is to see the rates at which demographic groups use different modes of transportation for commuting to their occupation. The raw data set contains all demographic differentiators in the same table, which would be classified as untidy data. Thus, it will be necessary to split these into several tables; one for each demographic type. Additionally, the columns denoting percent error are useful for understanding the data, but could be cumbersome for conducting EDA, so we will only be focusing on the proportions given in the data columns. \n",
    "\n",
    "Cleaning this data set will allow us to use `R` to clean qualitative, as well as quantitative variables. The following are steps for carrying this out:\n",
    "\n",
    "- Read full .csv file and rename columns for readability based on glossary given by the data source\n",
    "- Select only rows that split records by `age`, and only columns that contain data points\n",
    "- Trim leading spaces from `age` column\n",
    "- Remove percentage symbol from numeric fields and convert them to numeric data type\n",
    "- Repeat process for `sex`, `race`, `citizenship status`, and `earnings`\n",
    "\n",
    "Below is the code, and a sample screenshot from the `earnings` cleaned data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(tidyr)\n",
    "library(stringr)\n",
    "\n",
    "demographics <- read.csv(\"../data/ridership_by_demographic_2021.csv\")\n",
    "colnames(demographics)[c(2,4,6,8)] <- c(\"total\", \"drive_alone\", \"carpool\", \"public_transit\")\n",
    "age <- demographics[c(3:8), c(1,2,4,6,8)]\n",
    "colnames(age)[1] <- \"age_group\"\n",
    "age$age_group <- str_trim(age$age_group, \"left\")\n",
    "age$total <- as.numeric(substr(age$total, 1, nchar(age$total)-1))\n",
    "age$drive_alone <- as.numeric(substr(age$drive_alone, 1, nchar(age$drive_alone)-1))\n",
    "age$carpool <- as.numeric(substr(age$carpool, 1, nchar(age$carpool)-1))\n",
    "age$public_transit <- as.numeric(substr(age$public_transit, 1, nchar(age$public_transit)-1))\n",
    "head(age)\n",
    "write.csv(age, \"../data/cleaned_data/ridership_age.csv\")\n",
    "\n",
    "sex <- demographics[c(11:12), c(1,2,4,6,8)]\n",
    "colnames(sex)[1] <- \"sex\"\n",
    "sex$sex <- str_trim(sex$sex, \"left\")\n",
    "sex$total <- as.numeric(substr(sex$total, 1, nchar(sex$total)-1))\n",
    "sex$drive_alone <- as.numeric(substr(sex$drive_alone, 1, nchar(sex$drive_alone)-1))\n",
    "sex$carpool <- as.numeric(substr(sex$carpool, 1, nchar(sex$carpool)-1))\n",
    "sex$public_transit <- as.numeric(substr(sex$public_transit, 1, nchar(sex$public_transit)-1))\n",
    "head(sex)\n",
    "write.csv(sex, \"../data/cleaned_data/ridership_sex.csv\")\n",
    "\n",
    "citizenship <- demographics[c(25:28), c(1,2,4,6,8)]\n",
    "colnames(citizenship)[1] <- \"status\"\n",
    "citizenship$status <- str_trim(citizenship$status, \"left\")\n",
    "citizenship$total <- as.numeric(substr(citizenship$total, 1, nchar(citizenship$total)-1))\n",
    "citizenship$drive_alone <- as.numeric(substr(citizenship$drive_alone, 1, nchar(citizenship$drive_alone)-1))\n",
    "citizenship$carpool <- as.numeric(substr(citizenship$carpool, 1, nchar(citizenship$carpool)-1))\n",
    "citizenship$public_transit <- as.numeric(substr(citizenship$public_transit, 1, nchar(citizenship$public_transit)-1))\n",
    "head(citizenship)\n",
    "write.csv(citizenship, \"../data/cleaned_data/ridership_citizenship.csv\")\n",
    "\n",
    "earnings <- demographics[c(35:42), c(1,2,4,6,8)]\n",
    "colnames(earnings)[1] <- \"range\"\n",
    "earnings$range <- str_trim(earnings$range, \"left\")\n",
    "earnings$total <- as.numeric(substr(earnings$total, 1, nchar(earnings$total)-1))\n",
    "earnings$drive_alone <- as.numeric(substr(earnings$drive_alone, 1, nchar(earnings$drive_alone)-1))\n",
    "earnings$carpool <- as.numeric(substr(earnings$carpool, 1, nchar(earnings$carpool)-1))\n",
    "earnings$public_transit <- as.numeric(substr(earnings$public_transit, 1, nchar(earnings$public_transit)-1))\n",
    "head(earnings)\n",
    "write.csv(earnings, \"../data/cleaned_data/ridership_earnings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transportation Methods by Earnings - Cleaned](../images/ridership_earnings_cleaned.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "U.S. Census Bureau. \"MEANS OF TRANSPORTATION TO WORK BY SELECTED CHARACTERISTICS.\" American Community Survey, ACS 5-Year Estimates Subject Tables, Table S0802, 2021, https://data.census.gov/table/ACSST5Y2021.S0802?t=Commuting&g=860XX00US20020,20032. Accessed on October 12, 2023."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
